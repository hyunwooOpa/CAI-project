{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Conversational AI (25/26) Assignment 3: Evaluation\n",
    "This notebook focuses on the question of what it means to evaluate conversational AI and how this can be done in practice.\n",
    "The dataset used in the assignment is the one from [DSTC11's task 5](https://github.com/lkra/dstc11-track5/blob/main/data), same as assignments 1 and 2.\n",
    "\n",
    "\n",
    "## Assignment\n",
    "In this assignment, we will evaluate the vanilla and fine-tuned versions of a large language model (Qwen-2.5) using four automatic metrics and compare them to human judgments.\n",
    "\n",
    "**Assignment steps:**\n",
    "\n",
    "First, run this notebook and make sure you understand what is going on. Throughout the notebook, you will find nine questions you need to answer to complete the assignment. These are both coding questions and questions that evaluate your understanding of the data, the process, and the model behavior. These questions are indicated as <span style=\"background:yellow\">__Q#__, namely:</span>\n",
    "\n",
    "\n",
    "1. Understand the provided metrics: BLEU, Rouge, and BERTScore. Run them for the vanilla and the fine-tuned model for 100 dialogues. Reflect on how they perform in relation to their design. **(Q1 - Q3)**\n",
    "3. Define manual/qualitative metrics and compare them to the automatic metrics you generated so far. Analyze and reflect on what you observe. **(Q4 - Q5)**\n",
    "4. Complete the code snippet for the LLM-as-a-judge function. Compare its scores to the human scores and the scores of the other metrics. **(Q6 - Q9)**\n",
    "\n",
    "**Submission:**\n",
    "Please submit your code (as a Kaggle notebook) on Canvas by **17th November 23:59**.\n",
    "\n",
    "**Grading:** The assignment is graded with a pass/fail grade.\n",
    "\n",
    "## A brief reminder about Kaggle notebooks\n",
    "\n",
    "1. Pay attention to usage statistics, especially memory, CPU, and GPU\n",
    "2. Pay attention to the quota of GPU (measured in hours)\n",
    "3. \"Turn OFF\" the internet after each session. Turn on the internet when starting a session.\n",
    "4. \"Turn off\" the accelerator after each use. Turn the accelerator on when starting a session.\n",
    "5. Save a version after you make changes. This ensures that your teammate can see the latest changes. If you get a question from Kaggle about versions, you can revert to the latest version. With \"quick save\" you can save a version without running everything. However, while submitting the assignment, the outputs must be visible,\n",
    "\n",
    "## Ready?\n",
    "Let's get started! There are four kinds of preparations we will start with:\n",
    "1. installing packages\n",
    "2. importing relevant packages\n",
    "3. setting up the directory with the task data\n",
    "4. defining helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The Kaggle notebooks use a Python 3 environment, and they are already \"pre-loaded\" with various analytic Python packages, like Json, Pandas, and Numpy. If you are curious, you can see the package definition in [this repository](https://github.com/kaggle/docker-python).\n",
    "\n",
    "We will install a few specialized packages for evaluation and for working with language models.\n",
    "\n",
    "Let's just double check that Python is set up and we are using a relatively new version (like 3.10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.14.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries are relevant for our evaluation functions. As they are commonly used evaluation metrics, people have created standardized code that we can reuse. That way we won't need to define the evaluation comparisons ourselves.\n",
    "\n",
    "We also install `transformers`, the package needed to work with Large Language Models.\n",
    "\n",
    "For our NLI-based evaluation (step 2), we will use the `sentence-transformers` library, which is very handy for computing cosine similariries between sentences. Computing NLI scores is an abstraction over these similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: regex in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sacrebleu) (2025.11.3)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sacrebleu) (2.3.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-6.0.2-cp314-cp314-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-6.0.2-cp314-cp314-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [sacrebleu]/5\u001b[0m [sacrebleu]\n",
      "\u001b[1A\u001b[2KSuccessfully installed colorama-0.4.6 lxml-6.0.2 portalocker-3.2.0 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from rouge) (1.17.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (2.3.4)\n",
      "Requirement already satisfied: dill in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (2.3.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (4.57.1)\n",
      "Requirement already satisfied: numpy in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (2.3.4)\n",
      "Requirement already satisfied: requests in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (4.67.1)\n",
      "Collecting matplotlib (from bert_score)\n",
      "  Downloading matplotlib-3.10.7-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.0.0->bert_score) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers>=3.0.0->bert_score) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers>=3.0.0->bert_score) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers>=3.0.0->bert_score) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers>=3.0.0->bert_score) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers>=3.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert_score) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert_score)\n",
      "  Downloading contourpy-1.3.3-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert_score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert_score)\n",
      "  Downloading fonttools-4.60.1-cp314-cp314-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert_score)\n",
      "  Downloading kiwisolver-1.4.9-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib->bert_score)\n",
      "  Downloading pillow-12.0.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->bert_score)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->bert_score) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->bert_score) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->bert_score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->bert_score) (2025.10.5)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading matplotlib-3.10.7-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (363 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp314-cp314-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, bert_score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [bert_score]8\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bert_score-0.3.13 contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5\n",
      "Requirement already satisfied: transformers in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers) (2025.10.5)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (2.9.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading scikit_learn-1.7.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# To use BLEU and ROUGE\n",
    "!pip install sacrebleu\n",
    "!pip install rouge\n",
    "\n",
    "# To use BERTSCORE\n",
    "!pip install evaluate\n",
    "!pip install bert_score\n",
    "\n",
    "# To use ْQwen\n",
    "!pip install transformers\n",
    "\n",
    "# To calculate cosine similarities between sentences\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "The following code loads several standard packages, packages for evaluation, and packages for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge import Rouge\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "bleu_scorer = BLEU()\n",
    "rouge_scorer = Rouge()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "#my local directory for the assignment\n",
    "myDir = \"/home/song0409/Desktop/CAI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the data/task repository\n",
    "\n",
    "We start by cloning the data and task repository and changing the working directory to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dstc11-track5'...\n"
     ]
    }
   ],
   "source": [
    "def setup_repo(repo_url: str, repo_name: str, work_dir: str = myDir):\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # Remove repo if it exists\n",
    "    if os.path.exists(os.path.join(work_dir, repo_name)):\n",
    "        shutil.rmtree(os.path.join(work_dir, repo_name))\n",
    "    \n",
    "    # Clone repo\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    \n",
    "    # Move into repo/data\n",
    "    os.chdir(os.path.join(repo_name, \"data\"))\n",
    "\n",
    "\n",
    "setup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all files in the current directory iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output_schema.json\n",
      "./README.md\n",
      "./knowledge_aug_domain_reviews.json\n",
      "./knowledge_aug_reviews.json\n",
      "./knowledge.json\n",
      "./val/labels.json\n",
      "./val/logs.json\n",
      "./test/labels.json\n",
      "./test/logs.json\n",
      "./train/logs_bkp.json\n",
      "./train/labels.json\n",
      "./train/logs.json\n",
      "./train/bkp/labels.json\n",
      "./train/bkp/logs.json\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files that are vital to this notebook. `Logs` contains the conversation logs, whereas `labels` are the ground-truth responses by humans. Feel free to inspect the contents of these files at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('train/logs.json', 'r') as f:\n",
    "    contexts=json.load(f)\n",
    "\n",
    "with open('train/labels.json', 'r') as f:\n",
    "    labels=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the fine-tuned model from A2. In the previous assignment, we saved our trained model in the output/adapter directory. Let's load the fine-tuned model from there.\n",
    "\n",
    "**Note:** alternatively, you can download a fine-tuned model using this link: https://drive.google.com/file/d/1f4bSiE356aZaQzbHUh48SN0c2uedDOnA/view?usp=drive_link \n",
    "If you do so, upload the model using the Input -> Upload option in your sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/song0409/Desktop/CAI/dstc11-track5/data/outputs/adapter'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src = myDir + \"/week2/\"+\"outputs/adapter\"\n",
    "dst = myDir + \"/dstc11-track5/data/outputs/adapter\"\n",
    "\n",
    "shutil.copytree(src, dst, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path  = myDir+\"/week2/\"+\"outputs/adapter\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model = PeftModel.from_pretrained(model_base_for_adapter, adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with standard generative metrics\n",
    "\n",
    "BLEU, ROUGE, and BERTScore are three common metrics for the evaluation of machine-generated text. These were discussed during this week's lecture. Before we proceed with empirically exploring them, let's first reflect on what they measure.\n",
    "\n",
    "<span style=\"background:yellow\">__Q1:__ For each of the three metrics (BLEU, ROUGE, Bertscore), describe what it measures and how it does that in practice. For the \"how\" part, you can also explain through the metric's equation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE, BLEU, and Bertscore are already implemented in this notebook. **We suggest you spend a few minutes reading through their code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bleu_scorer = BLEU(effective_order=True)\n",
    "\n",
    "def compute_rouge(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    Returns the prediction and the reference for a single dialogue, returns their ROUGE score.\n",
    "    \"\"\"\n",
    "    score = rouge_scorer.get_scores(\n",
    "          hyps=hypothesis,\n",
    "          refs=reference,\n",
    "      )\n",
    "    return score[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "def compute_bleu(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    Returns the prediction and the reference for a single dialogue, returns their BLEU score.\n",
    "    \"\"\"\n",
    "    score = bleu_scorer.sentence_score(\n",
    "        hypothesis=hypothesis,\n",
    "        references=[reference]\n",
    "    )\n",
    "    return score.score / 100\n",
    "\n",
    "def compute_bertscore(predictions, references):\n",
    "    \"\"\"\n",
    "    Receives two lists of strings with equal length. Returns their pairwise Bertscores (precision, recall, F1 score).\n",
    "    \"\"\"\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function obtains the evaluation scores for all three metrics. For BERTScore we will use the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_answers(model_response, gt_response):\n",
    "    rouge = compute_rouge(model_response, gt_response)\n",
    "    bleu = compute_bleu(model_response, gt_response)\n",
    "    bert = compute_bertscore([model_response], [gt_response])[\"f1\"][0]\n",
    "\n",
    "    print(\"MODEL:\", model_response)\n",
    "    print(\"GROUND TRUTH:\", gt_response)\n",
    "    print(\"ROUGE:\", rouge, \"BLEU:\", bleu, \"BERTSCORE:\", bert)\n",
    "    print()\n",
    "\n",
    "    return rouge, bleu, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM functions\n",
    "\n",
    "* **get_qwen3_model and get_qwen2_model**: Load the Qwen models and assign them to global variables.\n",
    "* **query_qwen**: Get the output of the models for a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_qwen3_model = None\n",
    "_qwen3_tokenizer = None\n",
    "\n",
    "_qwen2_model = None\n",
    "_qwen2_tokenizer = None\n",
    "\n",
    "\n",
    "def get_qwen3_model():\n",
    "    global _qwen3_model, _qwen3_tokenizer\n",
    "    if _qwen3_model is None or _qwen3_tokenizer is None:\n",
    "        model_name = \"Qwen/Qwen3-1.7B\"\n",
    "        _qwen3_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        _qwen3_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    return _qwen3_model, _qwen3_tokenizer\n",
    "\n",
    "\n",
    "def get_qwen2_model():\n",
    "    global _qwen2_model, _qwen2_tokenizer\n",
    "    if _qwen2_model is None or _qwen2_tokenizer is None:\n",
    "        model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "        _qwen2_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        _qwen2_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    return _qwen2_model, _qwen2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def query_qwen(messages, model_name):\n",
    "    \n",
    "    if model_name == \"Qwen3-1.7B\":\n",
    "        model, tokenizer = get_qwen3_model()\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "        )\n",
    "        \n",
    "    elif model_name == \"Qwen2.5-3B\":\n",
    "        model, tokenizer = get_qwen2_model()\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=2000\n",
    "    )\n",
    "\n",
    "    output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **prepare_messages:**  builds the input message sequence. It first defines the system instruction that sets the chatbot’s role as an expert in hotels and restaurants, then constructs the dialogue context by iterating through the conversation history. Each utterance is labeled according to the speaker (“user” or “assistant”) and added to the message list, and it returns a structured list of messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_messages(history):\n",
    "    \"\"\"\n",
    "    Prepare the messages that the model will get as input. \n",
    "    Considers the primary model instruction, demonstrations for few-shot settings, and the history of the current chat. Returns the list of messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # We write a suitable instruction for the model.\n",
    "    instruction=f'You are a chatbot with expertise in hotels and restaurants.\\n'\n",
    "    \n",
    "    # Construct dialogue history\n",
    "    messages=[]\n",
    "    messages.append({\"role\": \"system\", \"content\": instruction})\n",
    "    for ut in history:\n",
    "        role = \"user\" if ut['speaker']==\"U\" else \"system\"\n",
    "        messages.append({\"role\": role, \"content\": ut['text']})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q2:__ Now call the functions *prepare_messages* and *query_qwen* with a sample input (prediction and reference) to understand how they work. Use the same (prediction, reference) pair for a fair comparison between the three metrics. For the prediction, use Qwen3-1.7B to generate the response.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can run the three metrics with a single model and example, we are ready to scale up. \n",
    "\n",
    "* **run_pipeline**: this function iterates over the dialogs until reaching n_dialogues. It builds the prompt (zero-shot or few-shot) and generates the model's response (query_qwen). Then, it evaluates that response with ROUGE and BLEU metrics. Finally, it saves predictions, references, and metrics in lists and prints the progress and results of each dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_pipeline(n_dialogues, model_name):\n",
    "    \n",
    "    all_bleus = []\n",
    "    all_rouges = []\n",
    "    all_bertscores = []\n",
    "    all_predictions = []\n",
    "\n",
    "    all_bleus_ft = []\n",
    "    all_rouges_ft = []\n",
    "    all_bertscores_ft = []\n",
    "    all_predictions_ft = []\n",
    "\n",
    "    all_references = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    counter_dialogues = 0\n",
    "    idx_dialogue = 0\n",
    "\n",
    "    while counter_dialogues < n_dialogues:\n",
    "        chat = contexts[idx_dialogue]\n",
    "        if 'response' not in labels[idx_dialogue]:\n",
    "            idx_dialogue += 1\n",
    "            continue\n",
    "\n",
    "        gt_response = labels[idx_dialogue][\"response\"]\n",
    "        counter_dialogues += 1\n",
    "        idx_dialogue += 1\n",
    "\n",
    "        all_references.append(gt_response)\n",
    "\n",
    "        print(\"Processing dialogue\", counter_dialogues)\n",
    "\n",
    "        messages = prepare_messages(chat)\n",
    "\n",
    "        model_response = query_qwen(messages, model_name)\n",
    "        print(\"BASE MODEL:\")\n",
    "        rouge, bleu, bert = evaluate_answers(model_response, gt_response)\n",
    "        \n",
    "        all_predictions.append(model_response)\n",
    "        all_bleus.append(bleu)\n",
    "        all_rouges.append(rouge)\n",
    "        all_bertscores.append(bert)\n",
    "\n",
    "        # TODO: query_ft needs to be implemented (you can reuse the code from assignment 2)\n",
    "        # This function should be used to query the fine-tuned model\n",
    "        model_response_ft = query_ft(messages) \n",
    "        print(\"FINE-TUNED MODEL:\")\n",
    "        rouge_ft, bleu_ft, bert_ft = evaluate_answers(model_response_ft, gt_response)\n",
    "\n",
    "        all_predictions_ft.append(model_response_ft)\n",
    "        all_bleus_ft.append(bleu_ft)\n",
    "        all_rouges_ft.append(rouge_ft)\n",
    "        all_bertscores_ft.append(bert_ft)\n",
    "\n",
    "        if isinstance(chat, list):\n",
    "            input_text = \" \".join([turn[\"text\"] for turn in chat if turn[\"speaker\"] == \"U\"])\n",
    "        else:\n",
    "            input_text = chat.get(\"text\", str(chat))\n",
    "        all_inputs.append(input_text)\n",
    "\n",
    "    return (\n",
    "        all_bleus, all_rouges, all_bertscores, all_predictions,\n",
    "        all_bleus_ft, all_rouges_ft, all_bertscores_ft, all_predictions_ft,\n",
    "        all_references, all_inputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_dialogues = 10\n",
    "model_name = \"\"\n",
    "\n",
    "# TODO: Call the `run_pipeline` function to obtain the evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q3:__ Using the `run_pipeline` function, generate responses for the first 100 dialogues with both the vanilla and the fine-tuned model, and evaluate them using BLEU, ROUGE, and BERTScore metrics. This allows us to say which version of the model is better according to these metrics. What do you observe? Which model scores better in general? Do the metrics agree?\n",
    "Note: as part of this question, you'd need to define the function `query_ft`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print all our metrics for this configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"QWEN BASE\")\n",
    "print(\"Mean BLEU:\", np.mean(bleus))\n",
    "print(\"Mean ROUGE:\", np.mean(rouges))\n",
    "print(\"Mean BERTScore:\", np.mean(bertscores))\n",
    "\n",
    "print(\"\\nFINE-TUNED\")\n",
    "print(\"Mean BLEU:\", np.mean(bleus_ft))\n",
    "print(\"Mean ROUGE:\", np.mean(rouges_ft))\n",
    "print(\"Mean BERTScore:\", np.mean(bertscores_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual evaluation\n",
    "\n",
    "In this block, we will simulate a human evaluation. We will do so on a smaller set of 10 dialogues.\n",
    "\n",
    "\n",
    "<span style=\"background:yellow\">__Q4:__ Now define three quality criteria that you will apply to evaluate manually, by consulting papers by [Deriu et al.](https://link.springer.com/article/10.1007/s10462-020-09866-x) and by [Howcroft et al.](https://aclanthology.org/2020.inlg-1.23/). Describe each of the metrics in turn, both in terms of what it captures and how it works.</span>\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```\n",
    "\n",
    "<span style=\"background:yellow\">__Q5:__ Analyze the generation with the vanilla and fine-tuned model using these three metrics. Which of the model variants seems to be of the highest quality according to your human assessment? Note that the automatic metrics are only a proxy, and your impression may or may not agree with them. Remember to give examples and include results when describing your findings.</span>\n",
    "\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge\n",
    "\n",
    "LLM-as-a-judge has emerged as a popular and controversial evaluation technique. Given how successful LLMs have become, do we even need metrics based on human heuristics or human judgment? Can't we just judge a response using an LLM?\n",
    "We will next investigate whether LLM-as-a-judge can reliably judge the generations of our model in a way that resembles manual and other automated metrics.\n",
    "\n",
    "The code below provides a function that evaluates an answer with LLM-as-a-judge. Read through the code to understand what it does and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **query_qwen_as_a_judge:** this function evaluates how coherent an assistant’s response is relative to the user’s original input. It first sets a strict system instruction requiring the model to act as a coherence judge and return a JSON object containing a score between 0.0 and 1.0 plus a brief explanation. The function prepends this instruction to the provided message(s), applies Qwen’s chat template, and loads Qwen3-1.7B. It then tokenizes the input, generates up to 512 new tokens, and decodes the output. A regular expression is used to extract the JSON block between <JSON> and </JSON> tags. If successful, the JSON is parsed and returned; otherwise, the function returns a dictionary with a None score and an error description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def query_qwen_as_a_judge(messages, model_name):\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "### Role Assignment\n",
    "You are a Coherence Evaluation Judge.\n",
    "Your job is to evaluate how coherent the **assistant’s response** is with respect to the **user’s request**, in the context of conversations about hotels.\n",
    "\n",
    "### Task Definition\n",
    "You must:\n",
    "1. Assign a **coherence score** from **0.0 to 1.0**\n",
    "2. Provide a **short explanation** (maximum 2 sentences)\n",
    "\n",
    "### Output Format (STRICT)\n",
    "Return ONLY:\n",
    "\n",
    "<JSON>\n",
    "{\n",
    "  \"coherence_score\": float between 0.0 and 1.0,\n",
    "  \"explanation\": \"brief rationale\"\n",
    "}\n",
    "</JSON>\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    messages_judge = [system_prompt] + messages\n",
    "\n",
    "    if model_name == \"Qwen3-1.7B\":\n",
    "        model, tokenizer = get_qwen3_model()\n",
    "    else:\n",
    "        model, tokenizer = get_qwen2_model()\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages_judge,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs, max_new_tokens=512\n",
    "    )\n",
    "    output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "    raw_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract JSON using regex\n",
    "    match = re.search(r\"<JSON>(.*?)</JSON>\", raw_output, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            return data\n",
    "        except:\n",
    "            return {\"coherence_score\": None, \"explanation\": \"JSON parse error\"}\n",
    "    \n",
    "    return {\"coherence_score\": None, \"explanation\": \"No JSON found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block computes coherence scores for all model predictions using Qwen as a judge. It loops through each pair of user inputs and generated responses, formats them into a single evaluation prompt, and sends the prompt to query_qwen_as_a_judge. The function returns a JSON object containing a coherence score, which is extracted and stored in the list scores_base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores_base = []\n",
    "model_name = \"Qwen3-1.7B\"  \n",
    "\n",
    "for the_input, prediction in zip(all_inputs, all_predictions):\n",
    "    messages = [{\"role\": \"user\",\n",
    "        \"content\": f\"User Input:\\n{the_input}\\n\\nAssistant Output:\\n{prediction}\"}]\n",
    "\n",
    "    result = query_qwen_as_a_judge(messages, model_name)\n",
    "    scores_base.append(result[\"coherence_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q6:__ Now write a brief code snippet to run the LLM-as-a-judge evaluation for two samples (from the dataset): one where you expect a higher and one where you expect a lower score. Analyze whether you agree with the judge scores.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q7:__ Now evaluate the same 10 responses from Q4+Q5 for the vanilla and the fine-tuned model using LLM-as-a-judge. Compare the judgments of LLM-as-a-judge to your own judgments for these dialogues. What do you conclude? Would LLM-as-a-judge be a good proxy for your human judgments on this task? Why/why not?</span>\n",
    "\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q8:__ Compare the judgments of all four automatic metrics for the 100 dialogues, and compute Spearmann correlation for each metric pair: (llm-judge, bertscore), (llm-judge, bleu), (bleu, rouge). </span>\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```\n",
    "\n",
    "<span style=\"background:yellow\">__Q9:__ Visualize the scores between each metric pair as a matrix. What do you observe? Which of the metrics agree and disagree the most? Connect your observations to your knowledge about what these metrics capture.</span>\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Your solution and visualization here"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_cai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
