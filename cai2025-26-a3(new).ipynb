{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Conversational AI (25/26) Assignment 3: Evaluation\nThis notebook focuses on the question of what it means to evaluate conversational AI and how this can be done in practice.\nThe dataset used in the assignment is the one from [DSTC11's task 5](https://github.com/lkra/dstc11-track5/blob/main/data), same as assignments 1 and 2.\n\n\n## Assignment\nIn this assignment, we will evaluate the vanilla and fine-tuned versions of a large language model (Qwen-2.5) using four automatic metrics and compare them to human judgments.\n\n**Assignment steps:**\n\nFirst, run this notebook and make sure you understand what is going on. Throughout the notebook, you will find nine questions you need to answer to complete the assignment. These are both coding questions and questions that evaluate your understanding of the data, the process, and the model behavior. These questions are indicated as <span style=\"background:yellow\">__Q#__, namely:</span>\n\n\n1. Understand the provided metrics: BLEU, Rouge, and BERTScore. Run them for the vanilla and the fine-tuned model for 100 dialogues. Reflect on how they perform in relation to their design. **(Q1 - Q3)**\n3. Define manual/qualitative metrics and compare them to the automatic metrics you generated so far. Analyze and reflect on what you observe. **(Q4 - Q5)**\n4. Complete the code snippet for the LLM-as-a-judge function. Compare its scores to the human scores and the scores of the other metrics. **(Q6 - Q9)**\n\n**Submission:**\nPlease submit your code (as a Kaggle notebook) on Canvas by **17th November 23:59**.\n\n**Grading:** The assignment is graded with a pass/fail grade.\n\n## A brief reminder about Kaggle notebooks\n\n1. Pay attention to usage statistics, especially memory, CPU, and GPU\n2. Pay attention to the quota of GPU (measured in hours)\n3. \"Turn OFF\" the internet after each session. Turn on the internet when starting a session.\n4. \"Turn off\" the accelerator after each use. Turn the accelerator on when starting a session.\n5. Save a version after you make changes. This ensures that your teammate can see the latest changes. If you get a question from Kaggle about versions, you can revert to the latest version. With \"quick save\" you can save a version without running everything. However, while submitting the assignment, the outputs must be visible,\n\n## Ready?\nLet's get started! There are four kinds of preparations we will start with:\n1. installing packages\n2. importing relevant packages\n3. setting up the directory with the task data\n4. defining helper functions","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Installation\n\nThe Kaggle notebooks use a Python 3 environment, and they are already \"pre-loaded\" with various analytic Python packages, like Json, Pandas, and Numpy. If you are curious, you can see the package definition in [this repository](https://github.com/kaggle/docker-python).\n\nWe will install a few specialized packages for evaluation and for working with language models.\n\nLet's just double check that Python is set up and we are using a relatively new version (like 3.10):","metadata":{}},{"cell_type":"code","source":"!python --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following libraries are relevant for our evaluation functions. As they are commonly used evaluation metrics, people have created standardized code that we can reuse. That way we won't need to define the evaluation comparisons ourselves.\n\nWe also install `transformers`, the package needed to work with Large Language Models.\n\nFor our NLI-based evaluation (step 2), we will use the `sentence-transformers` library, which is very handy for computing cosine similariries between sentences. Computing NLI scores is an abstraction over these similarity scores.","metadata":{}},{"cell_type":"code","source":"# To use BLEU and ROUGE\n!pip install sacrebleu\n!pip install rouge\n\n# To use BERTSCORE\n!pip install evaluate\n!pip install bert_score\n\n# To use ْQwen\n!pip install transformers\n\n# To calculate cosine similarities between sentences\n!pip install sentence-transformers","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports\nThe following code loads several standard packages, packages for evaluation, and packages for working with LLMs.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport json\nimport os\nimport shutil\nimport subprocess\n\nfrom sacrebleu.metrics import BLEU\nfrom rouge import Rouge\n\nfrom evaluate import load\n\nfrom sentence_transformers import CrossEncoder\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbleu_scorer = BLEU()\nrouge_scorer = Rouge()\nbertscore = load(\"bertscore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clone the data/task repository\n\nWe start by cloning the data and task repository and changing the working directory to that directory.","metadata":{}},{"cell_type":"code","source":"def setup_repo(repo_url: str, repo_name: str, work_dir: str = \"/kaggle/working\"):\n    os.chdir(work_dir)\n    \n    # Remove repo if it exists\n    if os.path.exists(os.path.join(work_dir, repo_name)):\n        shutil.rmtree(os.path.join(work_dir, repo_name))\n    \n    # Clone repo\n    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n    \n    # Move into repo/data\n    os.chdir(os.path.join(repo_name, \"data\"))\n\n\nsetup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's list all files in the current directory iteratively:","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('.'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are two files that are vital to this notebook. `Logs` contains the conversation logs, whereas `labels` are the ground-truth responses by humans. Feel free to inspect the contents of these files at this point.","metadata":{}},{"cell_type":"code","source":"with open('train/logs.json', 'r') as f:\n    contexts=json.load(f)\n\nwith open('train/labels.json', 'r') as f:\n    labels=json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-tuned model ","metadata":{}},{"cell_type":"markdown","source":"We will use the fine-tuned model from A2. In the previous assignment, we saved our trained model in the output/adapter directory. Let's load the fine-tuned model from there.\n\n**Note:** alternatively, you can download a fine-tuned model using this link: https://drive.google.com/file/d/1f4bSiE356aZaQzbHUh48SN0c2uedDOnA/view?usp=drive_link \nIf you do so, upload the model using the Input -> Upload option in your sidebar.","metadata":{}},{"cell_type":"code","source":" import shutil\n\nsrc = \"/kaggle/input/outputs/adapter\"\ndst = \"/kaggle/working/dstc11-track5/data/outputs/adapter\"\n\nshutil.copytree(src, dst, dirs_exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model_id = \"Qwen/Qwen3-1.7B\"\nadapter_path  = \"outputs/adapter\" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\nmodel_base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\nmodel_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\nimport torch\n\nmodel = PeftModel.from_pretrained(model_base_for_adapter, adapter_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation with standard generative metrics\n\nBLEU, ROUGE, and BERTScore are three common metrics for the evaluation of machine-generated text. These were discussed during this week's lecture. Before we proceed with empirically exploring them, let's first reflect on what they measure.\n\n<span style=\"background:yellow\">__Q1:__ For each of the three metrics (BLEU, ROUGE, Bertscore), describe what it measures and how it does that in practice. For the \"how\" part, you can also explain through the metric's equation.</span>","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"ROUGE, BLEU, and Bertscore are already implemented in this notebook. **We suggest you spend a few minutes reading through their code below.**","metadata":{}},{"cell_type":"code","source":"bleu_scorer = BLEU(effective_order=True)\n\ndef compute_rouge(hypothesis, reference):\n    \"\"\"\n    Returns the prediction and the reference for a single dialogue, returns their ROUGE score.\n    \"\"\"\n    score = rouge_scorer.get_scores(\n          hyps=hypothesis,\n          refs=reference,\n      )\n    return score[0][\"rouge-l\"][\"f\"]\n\ndef compute_bleu(hypothesis, reference):\n    \"\"\"\n    Returns the prediction and the reference for a single dialogue, returns their BLEU score.\n    \"\"\"\n    score = bleu_scorer.sentence_score(\n        hypothesis=hypothesis,\n        references=[reference]\n    )\n    return score.score / 100\n\ndef compute_bertscore(predictions, references):\n    \"\"\"\n    Receives two lists of strings with equal length. Returns their pairwise Bertscores (precision, recall, F1 score).\n    \"\"\"\n    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next function obtains the evaluation scores for all three metrics. For BERTScore we will use the F1 score.","metadata":{}},{"cell_type":"code","source":"def evaluate_answers(model_response, gt_response):\n    rouge = compute_rouge(model_response, gt_response)\n    bleu = compute_bleu(model_response, gt_response)\n    bert = compute_bertscore([model_response], [gt_response])[\"f1\"][0]\n\n    print(\"MODEL:\", model_response)\n    print(\"GROUND TRUTH:\", gt_response)\n    print(\"ROUGE:\", rouge, \"BLEU:\", bleu, \"BERTSCORE:\", bert)\n    print()\n\n    return rouge, bleu, bert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LLM functions\n\n* **get_qwen3_model and get_qwen2_model**: Load the Qwen models and assign them to global variables.\n* **query_qwen**: Get the output of the models for a given prompt.","metadata":{}},{"cell_type":"code","source":"_qwen3_model = None\n_qwen3_tokenizer = None\n\n_qwen2_model = None\n_qwen2_tokenizer = None\n\n\ndef get_qwen3_model():\n    global _qwen3_model, _qwen3_tokenizer\n    if _qwen3_model is None or _qwen3_tokenizer is None:\n        model_name = \"Qwen/Qwen3-1.7B\"\n        _qwen3_tokenizer = AutoTokenizer.from_pretrained(model_name)\n        _qwen3_model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n    return _qwen3_model, _qwen3_tokenizer\n\n\ndef get_qwen2_model():\n    global _qwen2_model, _qwen2_tokenizer\n    if _qwen2_model is None or _qwen2_tokenizer is None:\n        model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n        _qwen2_tokenizer = AutoTokenizer.from_pretrained(model_name)\n        _qwen2_model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n    return _qwen2_model, _qwen2_tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def query_qwen(messages, model_name):\n    \n    if model_name == \"Qwen3-1.7B\":\n        model, tokenizer = get_qwen3_model()\n\n        text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False\n        )\n        \n    elif model_name == \"Qwen2.5-3B\":\n        model, tokenizer = get_qwen2_model()\n\n        text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n        )\n    \n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=2000\n    )\n\n    output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n\n    return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **prepare_messages:**  builds the input message sequence. It first defines the system instruction that sets the chatbot’s role as an expert in hotels and restaurants, then constructs the dialogue context by iterating through the conversation history. Each utterance is labeled according to the speaker (“user” or “assistant”) and added to the message list, and it returns a structured list of messages.\n","metadata":{}},{"cell_type":"code","source":"def prepare_messages(history):\n    \"\"\"\n    Prepare the messages that the model will get as input. \n    Considers the primary model instruction, demonstrations for few-shot settings, and the history of the current chat. Returns the list of messages.\n    \"\"\"\n    \n    \n    # We write a suitable instruction for the model.\n    instruction=f'You are a chatbot with expertise in hotels and restaurants.\\n'\n    \n    # Construct dialogue history\n    messages=[]\n    messages.append({\"role\": \"system\", \"content\": instruction})\n    for ut in history:\n        role = \"user\" if ut['speaker']==\"U\" else \"system\"\n        messages.append({\"role\": role, \"content\": ut['text']})\n\n    return messages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q2:__ Now call the functions *prepare_messages* and *query_qwen* with a sample input (prediction and reference) to understand how they work. Use the same (prediction, reference) pair for a fair comparison between the three metrics. For the prediction, use Qwen3-1.7B to generate the response.</span>","metadata":{}},{"cell_type":"code","source":"# Your solution here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that you can run the three metrics with a single model and example, we are ready to scale up. \n\n* **run_pipeline**: this function iterates over the dialogs until reaching n_dialogues. It builds the prompt (zero-shot or few-shot) and generates the model's response (query_qwen). Then, it evaluates that response with ROUGE and BLEU metrics. Finally, it saves predictions, references, and metrics in lists and prints the progress and results of each dialog.","metadata":{}},{"cell_type":"code","source":"def run_pipeline(n_dialogues, model_name):\n    \n    all_bleus = []\n    all_rouges = []\n    all_bertscores = []\n    all_predictions = []\n\n    all_bleus_ft = []\n    all_rouges_ft = []\n    all_bertscores_ft = []\n    all_predictions_ft = []\n\n    all_references = []\n    all_inputs = []\n    \n    counter_dialogues = 0\n    idx_dialogue = 0\n\n    while counter_dialogues < n_dialogues:\n        chat = contexts[idx_dialogue]\n        if 'response' not in labels[idx_dialogue]:\n            idx_dialogue += 1\n            continue\n\n        gt_response = labels[idx_dialogue][\"response\"]\n        counter_dialogues += 1\n        idx_dialogue += 1\n\n        all_references.append(gt_response)\n\n        print(\"Processing dialogue\", counter_dialogues)\n\n        messages = prepare_messages(chat)\n\n        model_response = query_qwen(messages, model_name)\n        print(\"BASE MODEL:\")\n        rouge, bleu, bert = evaluate_answers(model_response, gt_response)\n        \n        all_predictions.append(model_response)\n        all_bleus.append(bleu)\n        all_rouges.append(rouge)\n        all_bertscores.append(bert)\n\n        # TODO: query_ft needs to be implemented (you can reuse the code from assignment 2)\n        # This function should be used to query the fine-tuned model\n        model_response_ft = query_ft(messages) \n        print(\"FINE-TUNED MODEL:\")\n        rouge_ft, bleu_ft, bert_ft = evaluate_answers(model_response_ft, gt_response)\n\n        all_predictions_ft.append(model_response_ft)\n        all_bleus_ft.append(bleu_ft)\n        all_rouges_ft.append(rouge_ft)\n        all_bertscores_ft.append(bert_ft)\n\n        if isinstance(chat, list):\n            input_text = \" \".join([turn[\"text\"] for turn in chat if turn[\"speaker\"] == \"U\"])\n        else:\n            input_text = chat.get(\"text\", str(chat))\n        all_inputs.append(input_text)\n\n    return (\n        all_bleus, all_rouges, all_bertscores, all_predictions,\n        all_bleus_ft, all_rouges_ft, all_bertscores_ft, all_predictions_ft,\n        all_references, all_inputs\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_dialogues = 10\nmodel_name = \"\"\n\n# TODO: Call the `run_pipeline` function to obtain the evaluation scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q3:__ Using the `run_pipeline` function, generate responses for the first 100 dialogues with both the vanilla and the fine-tuned model, and evaluate them using BLEU, ROUGE, and BERTScore metrics. This allows us to say which version of the model is better according to these metrics. What do you observe? Which model scores better in general? Do the metrics agree?\nNote: as part of this question, you'd need to define the function `query_ft`.\n</span>","metadata":{}},{"cell_type":"markdown","source":"Let's print all our metrics for this configuration:","metadata":{}},{"cell_type":"code","source":"print(\"QWEN BASE\")\nprint(\"Mean BLEU:\", np.mean(bleus))\nprint(\"Mean ROUGE:\", np.mean(rouges))\nprint(\"Mean BERTScore:\", np.mean(bertscores))\n\nprint(\"\\nFINE-TUNED\")\nprint(\"Mean BLEU:\", np.mean(bleus_ft))\nprint(\"Mean ROUGE:\", np.mean(rouges_ft))\nprint(\"Mean BERTScore:\", np.mean(bertscores_ft))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"## Manual evaluation\n\nIn this block, we will simulate a human evaluation. We will do so on a smaller set of 10 dialogues.\n\n\n<span style=\"background:yellow\">__Q4:__ Now define three quality criteria that you will apply to evaluate manually, by consulting papers by [Deriu et al.](https://link.springer.com/article/10.1007/s10462-020-09866-x) and by [Howcroft et al.](https://aclanthology.org/2020.inlg-1.23/). Describe each of the metrics in turn, both in terms of what it captures and how it works.</span>\n\n```\n# Your answer here\n```\n\n<span style=\"background:yellow\">__Q5:__ Analyze the generation with the vanilla and fine-tuned model using these three metrics. Which of the model variants seems to be of the highest quality according to your human assessment? Note that the automatic metrics are only a proxy, and your impression may or may not agree with them. Remember to give examples and include results when describing your findings.</span>\n\n\n```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"## LLM-as-a-judge\n\nLLM-as-a-judge has emerged as a popular and controversial evaluation technique. Given how successful LLMs have become, do we even need metrics based on human heuristics or human judgment? Can't we just judge a response using an LLM?\nWe will next investigate whether LLM-as-a-judge can reliably judge the generations of our model in a way that resembles manual and other automated metrics.\n\nThe code below provides a function that evaluates an answer with LLM-as-a-judge. Read through the code to understand what it does and how it works.","metadata":{}},{"cell_type":"markdown","source":"* **query_qwen_as_a_judge:** this function evaluates how coherent an assistant’s response is relative to the user’s original input. It first sets a strict system instruction requiring the model to act as a coherence judge and return a JSON object containing a score between 0.0 and 1.0 plus a brief explanation. The function prepends this instruction to the provided message(s), applies Qwen’s chat template, and loads Qwen3-1.7B. It then tokenizes the input, generates up to 512 new tokens, and decodes the output. A regular expression is used to extract the JSON block between <JSON> and </JSON> tags. If successful, the JSON is parsed and returned; otherwise, the function returns a dictionary with a None score and an error description.","metadata":{}},{"cell_type":"code","source":"import json\nimport re\n\ndef query_qwen_as_a_judge(messages, model_name):\n    system_prompt = {\n        \"role\": \"system\",\n        \"content\": \"\"\"\n### Role Assignment\nYou are a Coherence Evaluation Judge.\nYour job is to evaluate how coherent the **assistant’s response** is with respect to the **user’s request**, in the context of conversations about hotels.\n\n### Task Definition\nYou must:\n1. Assign a **coherence score** from **0.0 to 1.0**\n2. Provide a **short explanation** (maximum 2 sentences)\n\n### Output Format (STRICT)\nReturn ONLY:\n\n<JSON>\n{\n  \"coherence_score\": float between 0.0 and 1.0,\n  \"explanation\": \"brief rationale\"\n}\n</JSON>\n\"\"\"\n    }\n\n    messages_judge = [system_prompt] + messages\n\n    if model_name == \"Qwen3-1.7B\":\n        model, tokenizer = get_qwen3_model()\n    else:\n        model, tokenizer = get_qwen2_model()\n\n    text = tokenizer.apply_chat_template(\n        messages_judge,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n    generated_ids = model.generate(\n        **model_inputs, max_new_tokens=512\n    )\n    output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n    raw_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n\n    # Extract JSON using regex\n    match = re.search(r\"<JSON>(.*?)</JSON>\", raw_output, re.DOTALL)\n    if match:\n        json_str = match.group(1).strip()\n        try:\n            data = json.loads(json_str)\n            return data\n        except:\n            return {\"coherence_score\": None, \"explanation\": \"JSON parse error\"}\n    \n    return {\"coherence_score\": None, \"explanation\": \"No JSON found\"}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code block computes coherence scores for all model predictions using Qwen as a judge. It loops through each pair of user inputs and generated responses, formats them into a single evaluation prompt, and sends the prompt to query_qwen_as_a_judge. The function returns a JSON object containing a coherence score, which is extracted and stored in the list scores_base.","metadata":{}},{"cell_type":"code","source":"scores_base = []\nmodel_name = \"Qwen3-1.7B\"  \n\nfor the_input, prediction in zip(all_inputs, all_predictions):\n    messages = [{\"role\": \"user\",\n        \"content\": f\"User Input:\\n{the_input}\\n\\nAssistant Output:\\n{prediction}\"}]\n\n    result = query_qwen_as_a_judge(messages, model_name)\n    scores_base.append(result[\"coherence_score\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q6:__ Now write a brief code snippet to run the LLM-as-a-judge evaluation for two samples (from the dataset): one where you expect a higher and one where you expect a lower score. Analyze whether you agree with the judge scores.</span>","metadata":{}},{"cell_type":"code","source":"# Your solution here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q7:__ Now evaluate the same 10 responses from Q4+Q5 for the vanilla and the fine-tuned model using LLM-as-a-judge. Compare the judgments of LLM-as-a-judge to your own judgments for these dialogues. What do you conclude? Would LLM-as-a-judge be a good proxy for your human judgments on this task? Why/why not?</span>\n\n\n```\n# Your answer here\n```","metadata":{}},{"cell_type":"code","source":"# Your solution here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q8:__ Compare the judgments of all four automatic metrics for the 100 dialogues, and compute Spearmann correlation for each metric pair: (llm-judge, bertscore), (llm-judge, bleu), (bleu, rouge). </span>\n\n```\n# Your answer here\n```\n\n<span style=\"background:yellow\">__Q9:__ Visualize the scores between each metric pair as a matrix. What do you observe? Which of the metrics agree and disagree the most? Connect your observations to your knowledge about what these metrics capture.</span>\n\n```\n# Your answer here\n```","metadata":{}},{"cell_type":"code","source":"# Your solution and visualization here","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}