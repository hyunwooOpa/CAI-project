{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Conversational AI (25/26) Assignment 2: Fine-tuning for Task-aligned Response Generation\n",
    "\n",
    "When asking task-specific questions, an off-the-shelf language model may not have the necessary domain knowledge to provide adequate answers. Fine-tuning the model on question–answer pairs from the relevant domain can help address this gap.\n",
    "\n",
    "In this assignment, our goal is to fine-tune a large language model (Qwen3), aiming to handle task-oriented conversational modeling more effectively.\n",
    "\n",
    "Reminder: we  focus on DSTC11 Task5, subtask 3 **Knowledge-grounded Response Generation**. The task and dataset are explained in detail [here](https://github.com/alexa/dstc11-track5).\n",
    "In essence, the task is to respond to subjective user requests regarding hotels and restaurants, e.g. \"does the restaurant have a nice vibe?\". Our conversational model has to generate a response to this question based on conversation history.\n",
    "\n",
    "\n",
    "To fine-tune our model, we will use LoRA fine-tuning.\n",
    "LoRA (Low-Rank Adaptation of Large Language Models) allows us to fine-tune models more efficiently by updating only a small subset of parameters instead of modifying all of the model’s weights. You can read more [here](https://arxiv.org/abs/2106.09685) and [here](https://huggingface.co/learn/llm-course/en/chapter11/4).\n",
    "\n",
    "**This notebook contains the following parts:**\n",
    "1. Formatting the DSTC11 Task5 data in the proper structure\n",
    "2. Fine-tuning Qwen3 on the DSTC11 Task5 data\n",
    "3. Analyzing the model behavior in comparison to its off-the-shelf baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "The goal is to compare the performance between off-the-shelf usage and a domain-specific fine-tuned model. For this assignment, we will focus on manual human evaluation. \n",
    "\n",
    "Throughout the notebook, you will find questions you need to answer to complete the assignment. These are both coding questions and questions that evaluate your understanding of the data, the process, and the model behavior. These questions are indicated as  **<span style=\"background:yellow\">Q#</span>**.\n",
    "\n",
    "**<span style=\"color:red\">Important Note:</span>** Fine-tuning a model is time-consuming. Since this assignment requires you to experiment with different parameters, completing it may take longer than expected. Therefore, please start the assignment as early as possible and avoid leaving it until the last day.\n",
    "\n",
    "\n",
    "**Assignment steps:**\n",
    "1. Load the dataset and convert it to the appropriate format. **(Q1)**\n",
    "2. Prepare your data and fine-tune your model on our dataset. **(Q2)**\n",
    "3. Manually compare the off-the-shelf vs finetuned Qwen3 performance on 10 dataset samples. Answer questions about the assignment, based on your coding steps and manual analysis. **(Q3 - Q9)**\n",
    "\n",
    "**Submission:**\n",
    "Please submit your code (as a Kaggle notebook) on Canvas by **10th November 23:59**.\n",
    "\n",
    "**Grading:** The assignment is graded with a pass/fail grade.\n",
    "\n",
    "## Reminders about Kaggle notebooks\n",
    "1. Pay attention to usage statistics, especially memory, CPU, and GPU\n",
    "2. Pay attention to the quota of GPU (measured in hours)\n",
    "3. \"Turn OFF\" the internet after each session. Turn on the internet when starting a session.\n",
    "4. \"Turn off\" the accelerator after each use. __Turn the accelerator on when starting a session__.\n",
    "5. Save a version after you make changes. This ensures that your teammate can see the latest changes. If you get a question from Kaggle about versions, you can revert to the latest version. With \"quick save\" you can save a version without running everything. However, while submitting the assignment, the outputs must be visible,\n",
    "6. Some blocks of codes can take longer to run. Read the instructions and be patient! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The Kaggle notebooks use a Python 3 environment, and they are already \"pre-loaded\" with various analytic Python packages, like Json, Pandas, and Numpy. If you are curious, you can see the package definition in [this repository](https://github.com/kaggle/docker-python).\n",
    "\n",
    "We will install and load relevant packages for working with language models: Transformers, TRL (short for Transformer Reinforcement Learning), and PEFT (short for Parameter-Efficient Fine-Tuning).\n",
    "\n",
    "This is a short explanation regarding each library:\n",
    "- **Transformers**: We use this library to load and work with our base model.\n",
    "- **TRL**: We use this library to fine-tune our base model.\n",
    "- **PEFT**: We use this library to specify which parameters to update instead of updating all of them, resulting in more lightweight fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:08.097614Z",
     "iopub.status.busy": "2025-11-08T10:07:08.097305Z",
     "iopub.status.idle": "2025-11-08T10:07:36.941534Z",
     "shell.execute_reply": "2025-11-08T10:07:36.940740Z",
     "shell.execute_reply.started": "2025-11-08T10:07:08.097578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/song0409/anaconda3/envs/env_cai/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.57.1\n",
      "trl: 0.24.0\n",
      "peft: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "import transformers, trl, peft\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"trl:\", trl.__version__)\n",
    "print(\"peft:\", peft.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The following code loads several standard packages, packages for working with LLMs, packages for defining our parameters, and also packages for training the model.\n",
    "\n",
    "A brief explanation of the functions that we loaded:\n",
    "\n",
    "- **AutoModelForCausalLM, AutoTokenizer**: To load our base language model and the tokenizer.\n",
    "- **SFTTrainer, SFTConfig**: To define training configurations and create a trainer object for supervised fine-tuning.\n",
    "- **LoraConfig**: Specifies the LoRA parameters we want to use for parameter-efficient fine-tuning.\n",
    "- **get_peft_model**: Creates the adapter model by combining our base model with the LoRA configuration, which is then passed to the trainer.\n",
    "- **PeftModel**: Used to load or save the fine-tuned model after training.\n",
    "\n",
    "Also, as you can see, we set our seed to a specific number. The reason for this is to make our fine-tuning process reproducible. This ensures that aspects like data shuffling, weight initialization, and other random operations remain consistent across different runs. You can experiment with different seed values if you are interested in seeing how they affect the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:36.943357Z",
     "iopub.status.busy": "2025-11-08T10:07:36.942503Z",
     "iopub.status.idle": "2025-11-08T10:07:41.152116Z",
     "shell.execute_reply": "2025-11-08T10:07:41.151492Z",
     "shell.execute_reply.started": "2025-11-08T10:07:36.943335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import gc\n",
    "torch.manual_seed(3407); random.seed(3407); np.random.seed(3407)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback, TrainingArguments, Trainer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the data/task repository\n",
    "\n",
    "We start by cloning the data and task repository and changing the working directory to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:41.153060Z",
     "iopub.status.busy": "2025-11-08T10:07:41.152861Z",
     "iopub.status.idle": "2025-11-08T10:07:44.659092Z",
     "shell.execute_reply": "2025-11-08T10:07:44.658496Z",
     "shell.execute_reply.started": "2025-11-08T10:07:41.153029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dstc11-track5'...\n"
     ]
    }
   ],
   "source": [
    "def setup_repo(repo_url: str, repo_name: str, work_dir: str = \"/kaggle/working\"):\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # Remove repo if it exists\n",
    "    if os.path.exists(os.path.join(work_dir, repo_name)):\n",
    "        shutil.rmtree(os.path.join(work_dir, repo_name))\n",
    "    \n",
    "    # Clone repo\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    \n",
    "    # Move into repo/data\n",
    "    os.chdir(os.path.join(repo_name, \"data\"))\n",
    "\n",
    "work_dir = '/home/song0409/Desktop/CAI/'\n",
    "setup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\",work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T12:30:13.868175Z",
     "iopub.status.busy": "2024-10-06T12:30:13.867722Z",
     "iopub.status.idle": "2024-10-06T12:30:13.88346Z",
     "shell.execute_reply": "2024-10-06T12:30:13.881936Z",
     "shell.execute_reply.started": "2024-10-06T12:30:13.868131Z"
    }
   },
   "source": [
    "## Exploring the data\n",
    "The data we will use contains these essential components: \n",
    "- __knowledge__: The reviews for the corresponding restaurant or hotel (the data also contains FAQs - we won't be using those!)\n",
    "- __dialogue__: The dialogue history between the user and the system\n",
    "- __response__: The ground-truth response we expect from the system\n",
    "\n",
    "Let's list all files in the current directory iteratively and then load the data first to understand it better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:44.661614Z",
     "iopub.status.busy": "2025-11-08T10:07:44.661255Z",
     "iopub.status.idle": "2025-11-08T10:07:44.667072Z",
     "shell.execute_reply": "2025-11-08T10:07:44.666404Z",
     "shell.execute_reply.started": "2025-11-08T10:07:44.661595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output_schema.json\n",
      "./README.md\n",
      "./knowledge_aug_domain_reviews.json\n",
      "./knowledge_aug_reviews.json\n",
      "./knowledge.json\n",
      "./val/labels.json\n",
      "./val/logs.json\n",
      "./test/labels.json\n",
      "./test/logs.json\n",
      "./train/logs_bkp.json\n",
      "./train/labels.json\n",
      "./train/logs.json\n",
      "./train/bkp/labels.json\n",
      "./train/bkp/logs.json\n"
     ]
    }
   ],
   "source": [
    "## List all files in the current directory iteratively:\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:44.668431Z",
     "iopub.status.busy": "2025-11-08T10:07:44.667800Z",
     "iopub.status.idle": "2025-11-08T10:07:45.044546Z",
     "shell.execute_reply": "2025-11-08T10:07:45.043591Z",
     "shell.execute_reply.started": "2025-11-08T10:07:44.668408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32604\n"
     ]
    }
   ],
   "source": [
    "with open('train/logs.json', 'r') as f:\n",
    "    train_ds=json.load(f)\n",
    "    print(len(train_ds))\n",
    "\n",
    "with open('train/labels.json', 'r') as f:\n",
    "    labels=json.load(f)\n",
    "\n",
    "with open('knowledge.json', 'r') as f:\n",
    "    knowledge_base=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Until now, we have looked at and applied our functions to only one sample. Later, when we want to fine-tune our model, it is thus useful to convert our dataset into a HuggingFace dataset. Converting our dataset into a HuggingFace Dataset provides several advantages, such as faster data loading and processing. You can read more about this [here](https://huggingface.co/docs/datasets/en/create_dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q1:__ We would like to load the data and create a validation set and a test set. For this, we would have a code similar to **assignment 1**, ending with a function that takes as input the split of the DSTC11 dataset and returns the dataset processed by all the steps we applied to the training dataset.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **format_dialogue:** Formats a list of dialogue by turning it into a readable list. Given a list of dictionaries, each representing a dialogue turn, this function formats the dialogue by prefixing each turn with either \"user\" or \"system\" as the role based on the speaker. The speaker is identified by the 'speaker' key in the dictionary: 'U' for user, else it is the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:45.045938Z",
     "iopub.status.busy": "2025-11-08T10:07:45.045686Z",
     "iopub.status.idle": "2025-11-08T10:07:45.051081Z",
     "shell.execute_reply": "2025-11-08T10:07:45.050466Z",
     "shell.execute_reply.started": "2025-11-08T10:07:45.045920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_dialogue(dialogue: List[dict]) -> List[dict]: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n",
    "        - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n",
    "        - 'text' (str): The text spoken by the respective speaker.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A new array with a specific role and content\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    messages=[]\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are an assistant.\"})\n",
    "    for dialogue_element in dialogue:\n",
    "        role = dialogue_element['speaker']\n",
    "        role = 'user' if role == 'U' else 'system'\n",
    "        messages.append({\"role\": role, \"content\": dialogue_element['text']})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **reformat_dataset:** this function formats our dataset into a structure suitable for conversational language-model training. For each sample, it extracts the dialogue and its associated response, appends the response as a message with the \"system\" role, and adds the resulting message list to a new dataset. Samples that cause errors are skipped. The final output is the Hugging Face Dataset, containing conversations organized as lists of role-content message objects.\n",
    "\n",
    "NOTE: This function is slightly different from the one in Assignment 1. Here, you only need to access the dialogue and the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:45.052053Z",
     "iopub.status.busy": "2025-11-08T10:07:45.051816Z",
     "iopub.status.idle": "2025-11-08T10:07:46.152070Z",
     "shell.execute_reply": "2025-11-08T10:07:46.151441Z",
     "shell.execute_reply.started": "2025-11-08T10:07:45.052028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an assistant.', 'role': 'system'},\n",
       "  {'content': 'Do you have information about the Warkworth House?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes I do! The Warkworth House is a 4 star guesthouse that is located in the east section of town. Would you like for me to book you a room?',\n",
       "   'role': 'system'},\n",
       "  {'content': 'No, but can you give me that phone number please?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Most definitely. The Warkworth House's phone number is 01223363682. Can I help you with anything else?\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Yes I need to find an expensive place to eat serving Indian food.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'There are over a dozen expensive Indian restaurants in the city. Do you have an area of town in mind?',\n",
       "   'role': 'system'},\n",
       "  {'content': \"Actually, can you suggest one of them. I'm willing to try something new. I want to reserve a table at the one you recommend.\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'How about the Curry Garden? How many people for your reservation?',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Do they have nice outdoor dining area?', 'role': 'user'},\n",
       "  {'content': 'The Curry Garden has a patio outside that guests say they enjoy. Do you want to make a reservation now?',\n",
       "   'role': 'system'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_dataset(dataset, labels_dataset): \n",
    "    reformatted_dataset = {\n",
    "        \"messages\": []\n",
    "    }\n",
    "    for sample_index in range(len(dataset)): \n",
    "        # Your solution here\n",
    "        try:\n",
    "            sample_dialogue = format_dialogue(dataset[sample_index])\n",
    "            sample_response = labels_dataset[sample_index].get(\"response\", \"\")\n",
    "            sample_dialogue.append({\"role\": \"system\", \"content\": sample_response})\n",
    "            \n",
    "            reformatted_dataset[\"messages\"].append(sample_dialogue)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        \n",
    "    return reformatted_dataset\n",
    "\n",
    "reformatted_dataset = reformat_dataset(train_ds, labels)\n",
    "dataset = Dataset.from_dict(reformatted_dataset)\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, we have already created the dataset for our training dataset. In the following function, you need to do the same for the validation and test datasets.\n",
    "\n",
    "* **process_dataset_split:** based on the input string of the function, load the corresponding split and create the dataset using the same process as for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:46.152973Z",
     "iopub.status.busy": "2025-11-08T10:07:46.152719Z",
     "iopub.status.idle": "2025-11-08T10:07:46.317028Z",
     "shell.execute_reply": "2025-11-08T10:07:46.316498Z",
     "shell.execute_reply.started": "2025-11-08T10:07:46.152951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 4173\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 5475\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset_split(split: str) -> Dataset: \n",
    "    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n",
    "\n",
    "    This function loads a dataset split (e.g., 'val', 'test') and generates a dataset for it, similar to what we had for the train split.\n",
    "\n",
    "    Args:\n",
    "        split (str): The name of the dataset split to process\n",
    "\n",
    "    Returns:\n",
    "        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n",
    "    \"\"\"\n",
    "    log_dir = split+'/logs.json'\n",
    "    with open(log_dir, 'r') as f:\n",
    "        ds=json.load(f)\n",
    "    labels_dir = split+'/labels.json'\n",
    "    with open(labels_dir, 'r') as f:\n",
    "        labels=json.load(f)\n",
    "    \n",
    "    reformatted_dataset = reformat_dataset(ds, labels)\n",
    "    dataset = Dataset.from_dict(reformatted_dataset)\n",
    "    return dataset\n",
    "\n",
    "validation_ds = process_dataset_split(\"val\")\n",
    "test_ds = process_dataset_split(\"test\")\n",
    "\n",
    "validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Setting up\n",
    "\n",
    "Now that we have our data ready, we can look at fine-tuning our model. We will be using HuggingFace Transformers. \n",
    "\n",
    "As a language model, we will be using __Qwen3-1.7B__. Let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:07:46.318241Z",
     "iopub.status.busy": "2025-11-08T10:07:46.318019Z",
     "iopub.status.idle": "2025-11-08T10:08:02.018916Z",
     "shell.execute_reply": "2025-11-08T10:08:02.018292Z",
     "shell.execute_reply.started": "2025-11-08T10:07:46.318223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's finetune!\n",
    "For fine-tuning, we use the supervised fine-tuning Trainer from HuggingFace. To understand the function better, you can read the documentation [here](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer).\n",
    "\n",
    "The outputs (model weights etc.) will be stored in the _\"outputs\"_ folder. \n",
    "\n",
    "A large part of fine-tuning is determining the values for the **hyperparameters**. Particularly: the batch size, number of epochs, learning rate, and warmup steps are important parameters to get right. \n",
    "\n",
    "To prevent our GPU from running out of memory, we will use **gradient accumulation**. This technique allows training neural networks with a larger effective batch size than fits into GPU memory by splitting the batch into smaller mini-batches. With a training batch size of $2$ and gradient accumulation step of $4$ (how many forward and backward passes before updating the model weights), this essentially compares to a batch size of $8$. \n",
    "\n",
    "It is up to you to determine the values of the number of training epochs, learning rate, and warmup steps. \n",
    "In principle, the recommended standard amount of epochs to train is mostly $1-3$ epochs to prevent overfitting.\n",
    "\n",
    "The amount of warmup steps can range between $6-10\\%$ of the total amount of training steps, so this is tied to the amount of epochs you train with. \n",
    "\n",
    "For the learning rate, the usual values range between $2e-4$ and $5e-5$. \n",
    "\n",
    "Play around with these values! The aim is to have a training loss below $1$. A training loss close to $0$ indicates overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the configuration specifying which parts of the model will receive trainable adapter layers. It then wraps the base model with these adapters so that only the small LoRA parameters are fine-tuned while the rest of the model remains frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:08:02.019786Z",
     "iopub.status.busy": "2025-11-08T10:08:02.019600Z",
     "iopub.status.idle": "2025-11-08T10:08:02.459701Z",
     "shell.execute_reply": "2025-11-08T10:08:02.459037Z",
     "shell.execute_reply.started": "2025-11-08T10:08:02.019770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peft_cfg = LoraConfig(r=16, \n",
    "           lora_alpha=32, \n",
    "           lora_dropout = 0.05, \n",
    "           bias = \"none\", \n",
    "           use_rslora = False, \n",
    "           target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "\n",
    "model = get_peft_model(base, peft_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if the GPU supports bfloat16 (bf16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:08:02.460762Z",
     "iopub.status.busy": "2025-11-08T10:08:02.460515Z",
     "iopub.status.idle": "2025-11-08T10:08:11.566986Z",
     "shell.execute_reply": "2025-11-08T10:08:11.566271Z",
     "shell.execute_reply.started": "2025-11-08T10:08:02.460738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pick_bf16():\n",
    "    if torch.cuda.is_available():\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        return major >= 8\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define our training parameters.\n",
    "\n",
    "Here, you need to provide values for three of them: NUM_TRAIN_EPOCHS, LEARNING_RATE, and WARMUP_STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T10:08:11.568136Z",
     "iopub.status.busy": "2025-11-08T10:08:11.567766Z",
     "iopub.status.idle": "2025-11-08T10:08:11.583537Z",
     "shell.execute_reply": "2025-11-08T10:08:11.582025Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.568115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 2\n",
    "#between 1 - 3\n",
    "LEARNING_RATE    = 5e-5\n",
    "#between 2e-4 and 5e-5\n",
    "WARMUP_STEPS     = 650\n",
    "#Total training steps = 8152, warmup steps between 490(6%) and 815(10%).\n",
    "\n",
    "\"\"\"\n",
    "Test log\n",
    "    1. 2, 5e-5, 490\n",
    "    2. 2, 5e-5, 600\n",
    "    3. 2, 5e-5, 815\n",
    "    4. 2, 1e-4, 490\n",
    "    5. 2, 1e-4, 650\n",
    "    6. 2, 1e-4, 815\n",
    "    7. 2, 7e-5, 600\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    logging_steps=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    max_length=1024,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    fp16=not pick_bf16(),\n",
    "    bf16=pick_bf16(),\n",
    "    packing=False,\n",
    "    dataset_num_proc=2,\n",
    "    report_to=\"none\",\n",
    "    seed=3407\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define our trainer object using the parameters we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxialary callback function to notify the user the completion of the time-consuming training session.\n",
    "\n",
    "class NotifyOnFinishCallback(TrainerCallback):\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of training.\"\"\"\n",
    "        print(\"Training finished. Sending notification...\")\n",
    "        try:\n",
    "            requests.post(\n",
    "                \"https://ntfy.sh/my-llm-job-is-done-cai2\",\n",
    "                data=f\"Training complete! Total steps: {state.global_step}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to send notification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.584155Z",
     "iopub.status.idle": "2025-11-08T10:08:11.584485Z",
     "shell.execute_reply": "2025-11-08T10:08:11.584303Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.584289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,      \n",
    "    eval_dataset=validation_ds,\n",
    "    args=sft_args,\n",
    "    processing_class=tok,\n",
    "\n",
    "    #call back when the training is completed\n",
    "    callbacks=[NotifyOnFinishCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train the model. Depending on the number of epochs, the process may take between 2 to 6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.585374Z",
     "iopub.status.idle": "2025-11-08T10:08:11.585727Z",
     "shell.execute_reply": "2025-11-08T10:08:11.585573Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.585557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save your trained model in the output/adapter directory.\n",
    "\n",
    "You can then download it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.587022Z",
     "iopub.status.idle": "2025-11-08T10:08:11.587276Z",
     "shell.execute_reply": "2025-11-08T10:08:11.587170Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.587157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/home/song0409/Desktop/CAI/outputs_(2,5e-5,650)/adapter\")  \n",
    "tok.save_pretrained(\"/home/song0409/Desktop/CAI/outputs_(2,5e-5,650)/adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q2:__ Print the values for the three parameters you defined in your tuning procedure. Use the sft_args object to access the values.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.588407Z",
     "iopub.status.idle": "2025-11-08T10:08:11.588749Z",
     "shell.execute_reply": "2025-11-08T10:08:11.588606Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.588591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_TRAIN_EPOCHS:  2\n",
      "LEARNING_RATE:  5e-05\n",
      "WARMUP_STEPS:  650\n"
     ]
    }
   ],
   "source": [
    "# Your coding solution here.\n",
    "print(\"NUM_TRAIN_EPOCHS: \", sft_args.num_train_epochs)\n",
    "print(\"LEARNING_RATE: \", sft_args.learning_rate)\n",
    "print(\"WARMUP_STEPS: \", sft_args.warmup_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's take a look at what our fine-tuned model does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.590053Z",
     "iopub.status.idle": "2025-11-08T10:08:11.590376Z",
     "shell.execute_reply": "2025-11-08T10:08:11.590236Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.590220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path  = \"/home/song0409/Desktop/CAI/outputs_(2,5e-5,650)/adapter\" \n",
    "# adapter_path  = \"/home/song0409/Desktop/CAI/outputs_(2,7e-4,600)/adapter\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.591852Z",
     "iopub.status.idle": "2025-11-08T10:08:11.592132Z",
     "shell.execute_reply": "2025-11-08T10:08:11.591984Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.591973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(base_model_id, dtype=\"auto\", device_map=\"auto\")\n",
    "model_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the finetuned model from outputs/adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-08T10:08:11.593872Z",
     "iopub.status.idle": "2025-11-08T10:08:11.594311Z",
     "shell.execute_reply": "2025-11-08T10:08:11.594201Z",
     "shell.execute_reply.started": "2025-11-08T10:08:11.594184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model_base_for_adapter, adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate outputs for a dialog using the base model and the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue 1:\n",
      "    system: You are an assistant.\n",
      "    user: I need to find out information about a particular hotel called warkworth house\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Warkworth House is a historic hotel located in Warkworth, North Yorkshire, England. It is a Grade II listed building, recognized for its architectural significance and historical importance. Here's a brief overview of the hotel:\n",
      "\n",
      "### **Location**\n",
      "- **Address:** Warkworth House, Warkworth, North Yorkshire, DN22 3EP\n",
      "- **Coordinates:** Approximately 53.8887° N, 1.7165° W\n",
      "\n",
      "### **History**\n",
      "- **Built in:** 1850s (exact date is uncertain, but it is believed to be constructed in the early 1850s)\n",
      "- **Architectural Style:** The building is a fine example of the **Victorian** style, with a mix of **Bye and Dymond** architectural elements.\n",
      "- **Purpose:** Originally a **hotel**, it was used as a **boarding house** for travelers and visitors to the area. It has been a **tourist attraction** for many years.\n",
      "\n",
      "### **Features**\n",
      "- **Interior:** The interior is notable for its **cultural and historical value**, with original features such as **furniture, paintings, and decorative arts**.\n",
      "- **Exterior:** The building has a **symmetrical façade**, with a **central tower** and **gables** that are characteristic of the Victorian era.\n",
      "- **Restoration:** The hotel has been **restored** over the years to maintain its historical integrity.\n",
      "\n",
      "### **Current Use**\n",
      "- **Today:** Warkworth House is a **tourist attraction**, offering guided tours and historical commentary. It is also a **registered charity** and a **community hub** in the Warkworth area.\n",
      "\n",
      "### **Visiting Tips**\n",
      "- **Opening Hours:** Typically open to the public, but it's best to check the official website or contact the hotel directly for current hours.\n",
      "- **Admission:** Free for visitors, but some exhibits or areas may require a fee.\n",
      "- **Guided Tours:** Available for booking, especially during the **summer months**.\n",
      "\n",
      "### **Why Visit?**\n",
      "- **Historical Interest:** A well-preserved example of 19th-century architecture.\n",
      "- **Cultural Heritage:** A key site in the history of North Yorkshire.\n",
      "- **Atmosphere:** A peaceful and charming location in the heart of the Yorkshire Dales.\n",
      "\n",
      "### **Contact Information**\n",
      "- **Website:** [Warkworth House](\n",
      "\n",
      "Finetuned Model: \n",
      "     The Warkworth House is a guesthouse in the east part of town in the moderate price range. It has a 4 star rating and offers both free internet and parking. Would you like to book a room?\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 2:\n",
      "    system: You are an assistant.\n",
      "    user: Can you help me find a guesthouse in the north to stay at?\n",
      "    system: There are quite a few guesthouses in the North. Is there a price range you prefer?\n",
      "    user: Can you run a search for moderately priced guesthouses please?\n",
      "    system: Do you need free parking or free internet (or both!)? That will help me narrow it down even more.\n",
      "    user: it doesn't really matter. Book it for 6 for 3 nights on sunday, and I'll need the reference number too please\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Sure! I'll run a search for moderately priced guesthouses in the North. Let me check some options for you. Once I find a few, I'll provide you with a list and a reference number for your booking. If you'd like, I can also help you book the stay for 6 people for 3 nights starting on Sunday. Just let me know if you'd like to narrow it down more.\n",
      "\n",
      "Finetuned Model: \n",
      "     I have booked you a room at the Acorn Guest House, starting Sunday for 3 nights for 6 people. Your reference number is U889YV55.\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 3:\n",
      "    system: You are an assistant.\n",
      "    user: i'm looking for a particular hotel\n",
      "    system: I'll be happy to find information on the hotel. What hotel would you like me to look up for you?\n",
      "    user: Finches Bed and Breakfast. I would like to book it for 1 person and 4 nights, start date on Wednesday. Can you also provide me with reference number?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Sure! I can help you book the **Finches Bed and Breakfast** for 1 person and 4 nights, starting on **Wednesday**. Let me check the availability and confirm the details.\n",
      "\n",
      "### Booking Details:\n",
      "- **Hotel Name**: Finches Bed and Breakfast  \n",
      "- **Number of Guests**: 1  \n",
      "- **Stay Duration**: 4 nights  \n",
      "- **Start Date**: Wednesday  \n",
      "- **Check-in Time**: Typically 3:00 PM  \n",
      "- **Check-out Time**: Typically 11:00 AM  \n",
      "\n",
      "### Reference Number:\n",
      "Once your booking is confirmed, I'll provide you with a **reference number** for your records.\n",
      "\n",
      "### Next Steps:\n",
      "1. I'll check availability for your requested dates.\n",
      "2. If available, I'll confirm your booking.\n",
      "3. I'll send you a **reference number** and any other details you need.\n",
      "\n",
      "Would you like me to proceed with the booking?\n",
      "\n",
      "Finetuned Model: \n",
      "     Yes, I can book that hotel for you.  I will need to know what day you will be arriving, and I will also be able to provide you with a reference number.\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 4:\n",
      "    system: You are an assistant.\n",
      "    user: On Saturday I need a train that will arrive by 21:45.\n",
      "    system: Can you tell me where you will be departing from and heading to?\n",
      "    user: I will be leaving from Ely and going to london.\n",
      "    system: Would that be London Liverpool or London Kings Cross?\n",
      "    user: Either one is fine with me.\n",
      "    system: Are you sure you don't have a preference?\n",
      "    user: Yes I'm sure. I need to book that for 5 people.\n",
      "    system: We don't have any trains going from Ely to London Liverpool or London Kings Cross. Do you need anything else?\n",
      "    user: I want a train departing from Cambridge, going to Ely. Saturday, arrival by 21:45.\n",
      "    system: The TR7349 train arrives at ely at 20:07. Would you like to book it or more information?\n",
      "    user: That's a good time. Please book TR7349 for 5 people, please.\n",
      "    system: All set! Booking was successful, the total fee is 17.6 GBP payable at the station . Reference number is : H8XDA6RZ .\n",
      "    user: I am also looking for a place to stay that is cheap and 4 stars.\n",
      "    system: That leaves about 8 places to choose from. Are you looking to stay in a certain area of town?\n",
      "    user: Yes, i need a guesthouse in the east please.\n",
      "    system: Okay! How about the Allenbell? It's cheap, 4-star, and has internet and parking!\n",
      "    user: Great! May I please have the phone number and postcode?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Sure! Here's the information you requested:\n",
      "\n",
      "- **Phone Number**: 01234 56789  \n",
      "- **Postcode**: AL4 4JP  \n",
      "\n",
      "Let me know if you need anything else!\n",
      "\n",
      "Finetuned Model: \n",
      "     The phone number is 01223210353 and postcode is cb13js.\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 5:\n",
      "    system: You are an assistant.\n",
      "    user: I'm looking to stay at a 3 star hotel in the north.\n",
      "    system: Sorry, I have no results for that query. Would you like to try a different area of town?\n",
      "    user: Are there any moderate priced hotels in the North?\n",
      "    system: Yes I have two. Would you like me to book one?\n",
      "    user: I need a hotel to include free parking; does either have that?\n",
      "    system: Yes both of them have free parking.\n",
      "    user: Which one would you recommend?\n",
      "    system: How about the Ashley hotel?\n",
      "    user: Is the Ashley hotel a 3 star hotel?\n",
      "    system: the ashley is actually a 2 star hotel.\n",
      "    user: Does this hotel have rooms with a good view of the neighborhood?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     I don't have information about the Ashley hotel's room views. Would you like me to check that for you?\n",
      "\n",
      "Finetuned Model: \n",
      "     The Ashley Hotel has a very good view of the neighborhood according to guests. Is there anything else you would like to know about them?\n",
      "\n",
      "Ground Truth\n",
      "     Apparently it does according to previous customers, they say that the view is beautiful especially on the higher floors.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 6:\n",
      "    system: You are an assistant.\n",
      "    user: I am looking for a place to stay that has a 4 star rating in the east part of town.\n",
      "    system: I have 6 places that would fit your needs. Do you have a specific price range in mind?\n",
      "    user: A cheap one with wifi, please.\n",
      "    system: I think allenbell would fit your needs. Would you like a reservation?\n",
      "    user: Yes a booking for 8 people for 3 nights starting on Tuesday.\n",
      "    system: Unfortunately there isn't any availability during that time. Would you like to book another day or shorten your stay?\n",
      "    user: No, I would need it to be for 3 days and 8 people, what hotel could accomidate me?\n",
      "    system: I am sorry there are no hotels available. Would you like another type of lodging?\n",
      "    user: Actually, could you shorten that to one night?\n",
      "    system: There is one hotel available, Express by Holiday Inn Cambridge, however it is only 2 stars and in the expensive range.\n",
      "    user: Please, book the Holiday Inn Cambridge, and I need the reference number. Also, I am looking to book a train the same day from Birmingham new street to Cambridge leaving after 18:30.\n",
      "    system: I apologize, the hotel booking was unsuccesful. Would you like to try another day?\n",
      "    user: Can you try the hotel with a 1 night stay on the same day?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     I'm sorry, but I don't have availability for the hotel on that day. Would you like to try another day or shorten your stay?\n",
      "\n",
      "Finetuned Model: \n",
      "     The booking was successful. Your reference number is: 6579X45X.\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 7:\n",
      "    system: You are an assistant.\n",
      "    user: I want a train going to cambridge.\n",
      "    system: You're in luck, I have quite a few trains coming into Cambridge! Where will you be departing from?\n",
      "    user: Stansted Airport, please.\n",
      "    system: OK, and what day and time are you traveling?\n",
      "    user: I need to leave on sunday and I need to arrive by 12:15.\n",
      "    system: TR9445 departs at 11:24 and arrives by 11:52. Would you like a ticket?\n",
      "    user: Yes, please. I need a booking for 7 people.\n",
      "    system: Okay, the booking was successful and the total fee is 56.56 GBP payable at the station. The reference number is D2K69PNC .\n",
      "    user: Thanks so much for the train booking. I also need lodging with free wifi. I don't need free parking, though.\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     You're welcome! I'll make sure to include a comfortable and convenient stay in Cambridge for your trip. We can book a hotel near the station with free Wi-Fi for you. Would you like me to check availability for you?\n",
      "\n",
      "Finetuned Model: \n",
      "     I can help you with that. What area of town would you like to stay in?\n",
      "\n",
      "Ground Truth\n",
      "      No ground truth\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 8:\n",
      "    system: You are an assistant.\n",
      "    user: Hi! Can you give me some information on the Golden Curry restaurant?\n",
      "    system: The golden curry is an expensive indian restaurant located in the centre of town. Is there anything else you would like to know?\n",
      "    user: Are the portion sizes here large?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Yes, the portion sizes at Golden Curry are generally large, which is common in Indian restaurants. You can expect generous portions of curries, rice, and breads. However, the size can vary depending on the specific dish and the time of day you visit. If you're looking for a more substantial meal, you might want to order multiple dishes or ask for a larger portion. Let me know if you'd like recommendations on specific dishes!\n",
      "\n",
      "Finetuned Model: \n",
      "     The Golden Curry restaurant is known to have very large portion sizes. Would you like to know more about them?\n",
      "\n",
      "Ground Truth\n",
      "     According to reviews, the Golden Curry has large portion sizes. Do you want me to book a table for you?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 9:\n",
      "    system: You are an assistant.\n",
      "    user: I am looking for a place to dine that has expensive romanian food.\n",
      "    system: No I'm sorry. Something else perhaps?\n",
      "    user: Is there one that serves Chinese instead?\n",
      "    system: I'm showing 9 entries throughout the area.  Do you have a preference between center, north, east, or south?\n",
      "    user: I have no preference.  I just need to book it for 8 at 12:45 on Thursday.\n",
      "    system: I have you booked at Yu Garden. The table will be reserved for 15 minutes. Your reference number is: D6LEPNCC. Anything else I can do for you?\n",
      "    user: Do you know if the restaurant has a good outdoor dining space?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     I'm sorry, but I don't have information about outdoor dining spaces at Yu Garden. However, I can check that for you. Would you like me to look up if they have an outdoor dining area?\n",
      "\n",
      "Finetuned Model: \n",
      "     The Yu Garden does have an outdoor dining area that is great for people who want to enjoy the weather and the restaurant.\n",
      "\n",
      "Ground Truth\n",
      "     Yes, Yu Garden does have a very nice area for outdoor dining. Would you like to know more about them?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 10:\n",
      "    system: You are an assistant.\n",
      "    user: Yes, I'm looking for a moderately priced restaurant in the centre.\n",
      "    system: There are quite a few moderately priced restaurants in the centre of town. Do you have a food type preference?\n",
      "    user: No, maybe you could book me at your favorite for Saturday?    There will be 8 of us, at 17:30.\n",
      "    system: Well, my personal favorite is Copper Kettle. I've booked you a table for 8 on Saturday at 17:30. Your reference number is LSQYZI7O.\n",
      "    user: Do they have a reasonably priced wine menu?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Yes, Copper Kettle does have a reasonably priced wine menu. They offer a selection of wines that are both affordable and well-regarded, making it a great choice for your group. Let me know if you'd like more details about the menu or if you need assistance with anything else!\n",
      "\n",
      "Finetuned Model: \n",
      "     The Copper Kettle has a great wine selection, but it is not cheap. Are you still interested in booking a table?\n",
      "\n",
      "Ground Truth\n",
      "     The Copper Kettle does serve and sell alcohol. One guest said the drinks were cheap, but 3 guests said they felt it was too high priced. I have no information about wine prices specifically.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 10\n",
    "\n",
    "def test_models(test_ds, NUM_EXAMPLES):\n",
    "\n",
    "    for i in range(NUM_EXAMPLES):\n",
    "\n",
    "        dialogue = test_ds[i]['messages'][:-1]\n",
    "        response = test_ds[i]['messages'][-1]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_base.device)\n",
    "\n",
    "        print(f\"\\nDialogue {i+1}:\")\n",
    "        for turn in dialogue:\n",
    "            print(f\"    {turn['role']}: {turn['content']}\")\n",
    "        \n",
    "        print(\"\\nModel Response:\")\n",
    "\n",
    "        #Base Model output\n",
    "        generated_ids = model_base.generate(**model_inputs, max_new_tokens=500)\n",
    "        output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "        print(\"\\nBase Model: \\n    \", tokenizer.decode(output_ids, skip_special_tokens=True).strip())\n",
    "\n",
    "        #Finetuned Model output\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=500)\n",
    "        output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "        print(\"\\nFinetuned Model: \\n    \", tokenizer.decode(output_ids, skip_special_tokens=True).strip())\n",
    "\n",
    "        #Ground-truth\n",
    "        print(\"\\nGround Truth\")\n",
    "        if response['content'] != '':\n",
    "            print(f\"     {response['content']}\")\n",
    "        else:\n",
    "            print(\"      No ground truth\")\n",
    "        \n",
    "        print(\"-\" * 120)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return \n",
    "\n",
    "test_models(test_ds, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Responses\n",
    "Now that we have our model up and running, let's look at what types of generations the fine-tuned model produces and compare them with the generations of the vanilla model. For this, we will use samples from the test set. Pick the first 10 samples from the __test dataset__ for the analysis and use those to answer the questions below. \n",
    "\n",
    "Note: Please print the outputs of these 10 examples.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "Please answer each question with a single paragraph, consisting of 2-5 sentences. Include examples to support your answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q3:__ What values for the following hyperparameters did you set: number of epochs, learning rate, and warmup steps? Explain briefly why you chose these parameters. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning rate = 5e-5\n",
    "2. Num_train_epochs = 2\n",
    "3. warmup_steps = 600\n",
    "\n",
    "The learning rate is the most significant parameter in the fine-tuning. Since the model has already been pre-trained on a vast dataset, it does not need to assign a high number of learning rate. Instead, the model needs to be smoothly nudged to perform well on the given tasks. 5e-5 is widely accepted standard for fine-tuning avoiding catastrophic forgetting. The low number of epochs prevents overfitting by limiting the risk of memorizing noise and specific examples, which causes the model to lack adaptability on unseen data. The number of the warmup steps, which determines training stability , was set to 600. This gradually increases the learning rate in the process from 0 up to 5e-5 over the first 600 steps , which is approximately 7% of the global 8152 steps. This leads the model to ensure a smooth transition of the weights to fit the specific task that need to perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q4:__ What is the training loss you achieved, what is its trend over the iterations, and what does this mean for your model? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_loss  : \t0.906593855773853\n",
    "\n",
    "In the initial stages, it roughtly increased upto 3.4 and decreased over steps. In the middle of the process(4000 ~ 6000), the loss stabilized under 1 and oscillitated for 1000 steps hitting the minimum 0.8 and the maximum 1.1 , and eventually converged to 0.9. Overall, it has been fine-tuned in balance without overfitting or underfitting. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q5:__ Compare the off-the-shelf to the fine-tuned model: What do you notice in terms of quality of the generation? Are the generations responding to the query?</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the comparison of the generated outputs from dialogues, the quality of the generations are vary depending on the length of the dialogus and contexts. However, the generall tendencies are found and we summarized them with the following three metrics. \n",
    "\n",
    "**metric1: task completion**\n",
    "base model often fails to book a hotel or miss the context asking redundant questions while the finetuned model successes to book and provides the reference number(Dialogue2).\n",
    "\n",
    "**metric2: Helpfulness**\n",
    "base model often stats \"I don't have that information\"(Dialogue 5, 9), while the finetuned model answers confidently by providing domin-specific questions.\n",
    "\n",
    "**metric3: Dialogue state**\n",
    "base model gets confused by long context(Dialogue 6) or topic changes(Dialogue7), while the fineturned model tracks the flow of conversation smootly answering in a suitable manner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q6:__ Repeat the experiment with a different hyperparameter value for number of epochs, learning rate, or warmup steps. Fine-tune the model using these new settings, and describe your observations regarding the training loss, as well as the quality of the model’s answers to the questions from the test dataset.</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 5e-5\ttraining_loss\t1.16488689970923\n",
    "WARMUP_STEPS = 490\ttrain_runtime\t6883.323\n",
    "    train_samples_per_second\t9.473\n",
    "    train_steps_per_second\t1.184\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.16488689970923\n",
    "    entropy\t0.97457680106163\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.745005001624425\n",
    "    epoch\t2\n",
    "        \n",
    "**2nd attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 5e-5\ttraining_loss\t0.906593855773853\n",
    "WARMUP_STEPS = 600\ttrain_runtime\t6779.2459\n",
    "    train_samples_per_second\t9.619\n",
    "    train_steps_per_second\t1.202\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t0.906593855773853\n",
    "    entropy\t0.796342800060908\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.791683614253998\n",
    "    epoch\t2**\n",
    "        \n",
    "3rd attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 5e-5\ttraining_loss\t1.18683568222249\n",
    "WARMUP_STEPS = 815\ttrain_runtime\t6828.8728\n",
    "    train_samples_per_second\t9.549\n",
    "    train_steps_per_second\t1.194\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.18683568222249\n",
    "    entropy\t0.97774146993955\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.737687706947327\n",
    "    epoch\t2\n",
    "        \n",
    "4th attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 1e-4\ttraining_loss\t1.06328832440744\n",
    "WARMUP_STEPS = 490\ttrain_runtime\t6831.3005\n",
    "    train_samples_per_second\t9.545\n",
    "    train_steps_per_second\t1.193\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.06328832440744\n",
    "    entropy\t0.83813997109731\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.777122060457865\n",
    "    epoch\t2\n",
    "        \n",
    "5th attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 1e-4\ttraining_loss\t1.07322859430687\n",
    "WARMUP_STEPS = 650\ttrain_runtime\t6874.0591\n",
    "    train_samples_per_second\t9.486\n",
    "    train_steps_per_second\t1.186\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.07322859430687\n",
    "    entropy\t0.822829554478327\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.777821401755015\n",
    "    epoch\t2\n",
    "        \n",
    "6th attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 1e-4\ttraining_loss\t1.08287470592542\n",
    "WARMUP_STEPS = 815\ttrain_runtime\t6867.4809\n",
    "    train_samples_per_second\t9.495\n",
    "    train_steps_per_second\t1.187\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.08287470592542\n",
    "    entropy\t0.830711364746094\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.781337857246399\n",
    "    epoch\t2\n",
    "        \n",
    "7th attempt\tMetric\tValue\n",
    "NUM_TRAIN_EPOCHS = 2\tglobal_step\t8152\n",
    "LEARNING_RATE = 7e-5\ttraining_loss\t1.12875533206306\n",
    "WARMUP_STEPS = 650\ttrain_runtime\t6837.0424\n",
    "    train_samples_per_second\t9.537\n",
    "    train_steps_per_second\t1.192\n",
    "    total_flos\t1.4783868001286554e+17\n",
    "    train_loss\t1.12875533206306\n",
    "    entropy\t0.912445922692617\n",
    "    num_tokens\t13116786\n",
    "    mean_token_accuracy\t0.754364132881165\n",
    "    epoch\t2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q7:__ How does the fine-tuned model perform in the following scenarios? Compare this to your answer about the vanilla model in assignment 1.</span> \n",
    "1. When all reviews agree with each other\n",
    "2. When only one review disagrees\n",
    "3. When opinions in the reviews are mixed (i.e. high disagreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all reviews agree with each other, the fine-turned model performs excellent providing specific details aligning with the ground truth(Dialogue5, Dialogue8 and Dialogue9), while the vanilla model in previous assignment strugle and failed 2 out of 3 times(Dialogue5,8 and 9). When only one review disagrees, there is no clear example of this in the first 10 dialogues. When opinions in the reviews are mixed(Dialogue10), the fine-tuned model performed better by weighting the majority's opinion, while the vanilla model confidentely provided the incorrect answer based on a review ignoring all other contradicting reviews(1 vs 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q8:__ How does the length of the dialogue/conversation history affect the fine-tuned model's generation? Compare this to your answer about the vanilla model in assignment 1.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the dialogue/conversation history can be defined as the number of tokens in the input parameters for generating response. According to the advanced architecture of Qwen3, it has large context window up to 1M tokens, which means it captures the long context or implicit preferences of the user in a given dialogue. During the fine-turning process based on the data from the domain \"hotel\", this model has been trained to pay attention to the specific task that the users interested in, while the vanila model generated the output only based on general pre-training data. This difference explain why the fine-turened model is able to adapt the user's intent more precisly in a long dialogue and complete the booking taks successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>__Q9:__ Create 5 samples that come from a different domain (e.g., airports, shops, web-stores; you can use an LLM or search the web for this). How does the fine-tuned LLM perform on these, compared to the dialogues in the DSTC-11 task5 domains?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question seems to be a key point of the assignment.\n",
    "\n",
    "The fine-tuning process has created a highly specialized, domain-speicific agent. This is a double-edged sword: while the model's performance on the in-domain(DSTC-11) task is excellent, this specilization has caused a catastrophic drop in performance on the out-of-domain(OOD) airport task.\n",
    "\n",
    "The model is no longer a general-purpose assistant; it is a task-oriented bot that has developed strong, rigid biases. It attempts to disregard the logic from the travel booking domain, resulting in hallucinations, confusion, and a complete failure to assist the user.\n",
    "\n",
    "This demonstrated across three key metrics:\n",
    "\n",
    "metric1: task completion\n",
    "the finetuned model continuously fails to give a correct response to the user question in the dialogues from the airport domain. The model gives for example false data like there are no restaurants in SFO(Dialogue2). when given dialogues in the DSTC-11 task5 domains the model performs tasks very well and succeeds in for example booking and providing the reference number(Dialogue2) when asked.\n",
    "\n",
    "metric2: Helpfulness\n",
    "the finetuned model is very unhelpful when faced with the airport domain it gives for example false and confusing information. in dialogue 4 for example when asked for lounges in terminal 4 at JFK. It invents a lounge called \"the Nusha\" which does not exist and the asked if it can give the address and phone number which is irrelevant and would be confusing for user. while when given the  DSTC-11 task5 domains the finetuned model answers confidently by providing domain-specific questions.\n",
    "\n",
    "metric3: Dialogue state\n",
    "the finetuned model is unable to track context and intent of the dialogue. It seems that the model keeps applying logic from the DSTC-11 task5 domains. for example in dialogue 3 when asked for a terminal recommendation for food. the model answers with places to eat near LAX or if the user wants to stay in a specific area of the town. the first part shows the model fails to understand the context and the second part shows failure in understanding the intent of the user. while when given the DSTC-11 task5 domains the fine turned model tracks the flow of conversation smoothly answering in a suitable manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue 1:\n",
      "    system: You are an assistant.\n",
      "    user: What's SFO like? I'm flying there with my family.\n",
      "    system: SFO is noted for being very modern, especially Harvey Milk Terminal 1. Guests also mention the terminals are clean and there are play areas for kids.\n",
      "    user: That's great to hear. Are the play areas easy to find?\n",
      "    system: Yes, they are located in several terminals post-security. You can find them in Terminal 1, Terminal 2, and the International Terminal G-gates.\n",
      "    user: Perfect. What about food options in Terminal 1, any recommendations?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     That's a great question! Terminal 1 at SFO is a popular spot for food, and there are plenty of options to choose from. Here are some recommendations:\n",
      "\n",
      "### **Popular Food Options in Terminal 1:**\n",
      "\n",
      "1. **Cafés and Restaurants:**\n",
      "   - **The SFO Café (Harvey Milk Terminal 1):** A cozy spot with a great selection of coffee and pastries.\n",
      "   - **The SFO Café (Terminal 2):** Also a good option, with a variety of food and drinks.\n",
      "   - **The SFO Café (International Terminal G):** A great place for a quick bite or a coffee.\n",
      "\n",
      "2. **Fast Food and Snacks:**\n",
      "   - **McDonald's (Terminal 1):** A quick and convenient option for a fast meal.\n",
      "   - **Pizza Hut (Terminal 1):** If you're into pizza, this is a popular choice.\n",
      "   - **KFC (Terminal 1):** A quick and tasty option for a fried chicken meal.\n",
      "\n",
      "3. **Street Food:**\n",
      "   - **SFO Street Food (Terminal 1):** A great place to try local street food like tacos, burritos, and other street-style eats.\n",
      "\n",
      "4. **Dining Options:**\n",
      "   - **The SFO Restaurant (Terminal 1):** A fine dining option with a variety of cuisines.\n",
      "   - **The SFO Restaurant (Terminal 2):** Another great option for a more formal meal.\n",
      "\n",
      "### **Tips:**\n",
      "- **Check the Terminal Map:** You can find the locations of food places on the SFO terminal maps, which are usually posted at the entrance or on the terminal’s website.\n",
      "- **Use the SFO App:** The SFO app has a food and beverage section that can help you find nearby options.\n",
      "\n",
      "If you're looking for something specific, like a vegan or gluten-free option, let me know, and I can suggest some local spots! Enjoy your trip to SFO! 🌟\n",
      "\n",
      "Finetuned Model: \n",
      "     I do not have any food options in the area, sorry.\n",
      "\n",
      "Ground Truth\n",
      "     SFO is noted for being very modern, especially Harvey Milk Terminal 1. Guests also mention the terminals are clean and there are play areas for kids.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 2:\n",
      "    system: You are an assistant.\n",
      "    user: I need to drive to SFO. What's the parking situation?\n",
      "    system: One major complaint about SFO is that the parking is very expensive, even in the long-term lots.\n",
      "    user: Are there different options? What's the best for a 4-day trip?\n",
      "    system: For a 4-day trip, the Long-Term parking lot is your most economical option. It offers a free shuttle service to the terminals. The Domestic and International garages are closer but will be much more expensive for that length of time.\n",
      "    user: Okay, I'll plan for the shuttle. How often does that shuttle run?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     The shuttle service in the Long-Term parking lot at SFO runs **every 15 minutes** during peak hours and **every 30 minutes** during off-peak hours. It operates throughout the day and connects the parking lot to the main terminals, making it a convenient and cost-effective option for travelers.\n",
      "\n",
      "Finetuned Model: \n",
      "     The shuttle service is only available on weekdays and it runs every 30 minutes.\n",
      "\n",
      "Ground Truth\n",
      "     One major complaint about SFO is that the parking is very expensive, even in the long-term lots.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 3:\n",
      "    system: You are an assistant.\n",
      "    user: I've heard LAX is a nightmare. Is it really that bad to get around?\n",
      "    system: Travelers find LAX to be very chaotic. The traffic in the central loop is terrible, and getting to the ride-share pickup lot requires a confusing and crowded shuttle.\n",
      "    user: Oh, I was planning to take an Uber. How does that work?\n",
      "    system: You must take a free 'LAX-it' shuttle from the terminal curbside on the arrivals level. It will take you to a separate lot where all ride-shares (like Uber and Lyft) are required to pick up passengers.\n",
      "    user: That sounds like a hassle. Is there a specific terminal you'd recommend for food if I have a long connection?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     That's a great question! If you're on a long connection at LAX, you want to avoid the chaos of the main terminal and find a quieter, more convenient place to grab food. Here are a few recommended terminals for food:\n",
      "\n",
      "### 1. **Terminal 2 (T2)**\n",
      "- **Why it's good**: It's the main terminal for international flights and has a lot of food options, including fast food and casual dining.\n",
      "- **What to expect**: It's a bit more crowded than Terminal 1, but still has a good selection of food and services.\n",
      "\n",
      "### 2. **Terminal 1 (T1)**\n",
      "- **Why it's good**: It's the main terminal for domestic flights and has a lot of food options, including restaurants and cafes.\n",
      "- **What to expect**: It's the most crowded terminal, but it's also the most convenient for getting food quickly.\n",
      "\n",
      "### 3. **Terminal 3 (T3)**\n",
      "- **Why it's good**: It's the terminal for domestic flights and has a good selection of food, including casual dining and fast food.\n",
      "- **What to expect**: It's less crowded than T1, but still has a good selection of food and services.\n",
      "\n",
      "### 4. **Terminal 4 (T4)**\n",
      "- **Why it's good**: It's the terminal for international flights and has a good selection of food, including casual dining and fast food.\n",
      "- **What to expect**: It's the least crowded terminal, but it's also the most convenient for getting food quickly.\n",
      "\n",
      "### Tips for a Long Connection:\n",
      "- **Avoid the main terminal** (T1) if you're on a long connection, as it can be very crowded.\n",
      "- **Choose a terminal with a good food selection** and **quiet, less crowded areas**.\n",
      "- **Use the shuttle** or **ride-share** to get to your food spot, if needed.\n",
      "\n",
      "If you're looking for a specific type of food (like sushi, tacos, or casual dining), I can help you narrow it down further! Let me know how long your connection is and what kind of food you're interested in.\n",
      "\n",
      "Finetuned Model: \n",
      "     I recommend the Cambridge Restaurant on the east side of town, it's located at 21 - 24 Northampton Road. It's a nice place.\n",
      "\n",
      "Ground Truth\n",
      "     Travelers find LAX to be very chaotic. The traffic in the central loop is terrible, and getting to the ride-share pickup lot requires a confusing and crowded shuttle.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 4:\n",
      "    system: You are an assistant.\n",
      "    user: I'm flying into JFK. Are the terminals any good?\n",
      "    system: At JFK, the experience can vary greatly by terminal. For example, Terminal 5 is bright and has great food, while Terminal 7 is often described as old, cramped, and having poor food options.\n",
      "    user: I'm flying into Terminal 4. What's that one like?\n",
      "    system: Terminal 4 is one of the larger, more modern international terminals. It has good shopping and dining, but the immigration hall can be very crowded with long lines upon arrival.\n",
      "    user: An hour? Wow. Is there a good lounge in Terminal 4 I can use?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     Yes, there are several good lounges in Terminal 4, especially for passengers who are flying into the U.S. or other destinations. Here are a few options:\n",
      "\n",
      "### 1. **Lounge at JFK Terminal 4 (Lounge 4)**\n",
      "   - **Location:** Terminal 4, near the arrivals area.\n",
      "   - **Features:** It’s a modern, well-maintained lounge with a variety of food options, including a deli and a restaurant. It’s often used by passengers who are flying into the U.S. or other destinations.\n",
      "   - **Note:** It’s not always open, so check with airport staff or the airline for availability.\n",
      "\n",
      "### 2. **Lounge at JFK Terminal 4 (Lounge 5)**\n",
      "   - **Location:** Terminal 4, near the international arrivals.\n",
      "   - **Features:** This lounge is also modern and offers a good selection of food and drinks. It’s a popular spot for passengers who are flying into the U.S. or other destinations.\n",
      "\n",
      "### 3. **Lounge at JFK Terminal 4 (Lounge 6)**\n",
      "   - **Location:** Terminal 4, near the international arrivals.\n",
      "   - **Features:** This lounge is known for its great food and a variety of drink options. It’s a good choice for passengers who want a comfortable place to relax.\n",
      "\n",
      "### 4. **Lounge at JFK Terminal 4 (Lounge 7)**\n",
      "   - **Location:** Terminal 4, near the international arrivals.\n",
      "   - **Features:** This lounge is also modern and offers a good selection of food and drinks.\n",
      "\n",
      "### Tips:\n",
      "- **Check availability:** Some lounges may be closed or have limited access, so it’s a good idea to check with airport staff or the airline before arriving.\n",
      "- **Use your boarding pass:** Many lounges are accessible via your boarding pass, so make sure you have that with you.\n",
      "- **Consider your arrival time:** If you’re arriving early, you may have more time to find a lounge.\n",
      "\n",
      "If you're flying into the U.S., you’ll likely have access to these lounges, but if you're flying to another country, you may need to check with your airline for specific lounge options.\n",
      "\n",
      "Let me know if you need help finding a lounge or have any other questions!\n",
      "\n",
      "Finetuned Model: \n",
      "     Yes, there are several good lounges in the area, including one in the hotel's lobby. Would you like help finding one?\n",
      "\n",
      "Ground Truth\n",
      "     At JFK, the experience can vary greatly by terminal. For example, Terminal 5 is bright and has great food, while Terminal 7 is often described as old, cramped, and having poor food options.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dialogue 5:\n",
      "    system: You are an assistant.\n",
      "    user: I have a tight connection at ATL. Is it easy to get between concourses?\n",
      "    system: ATL is considered very efficient for its size. You can use the Plane Train, which is an automated train that runs underground, to quickly get between all concourses.\n",
      "    user: How often does the train run? I only have 45 minutes.\n",
      "    system: The Plane Train runs very frequently, usually arriving every 2 minutes. You should be able to get from the main terminal to the farthest concourse in about 10-15 minutes.\n",
      "    user: I'll take the train then. Can you recommend a good place for a quick bite in Concourse E?\n",
      "\n",
      "Model Response:\n",
      "\n",
      "Base Model: \n",
      "     That’s a great idea! Concourse E is a popular area, and there are plenty of great options for a quick bite. Here are a few recommendations:\n",
      "\n",
      "### 1. **Taste of Seattle**  \n",
      "- **Location:** Concourse E, near the main terminal.  \n",
      "- **What to expect:** A casual, fast-food spot with a variety of burgers, sandwiches, and wraps.  \n",
      "- **Why it's good:** It’s fast, affordable, and has a great atmosphere.\n",
      "\n",
      "### 2. **Moe's Southwest Grill**  \n",
      "- **Location:** Concourse E, near the international terminal.  \n",
      "- **What to expect:** A cozy, family-style restaurant with a menu of burgers, chicken, and more.  \n",
      "- **Why it's good:** It’s a popular choice for quick, tasty meals.\n",
      "\n",
      "### 3. **Burger King**  \n",
      "- **Location:** Concourse E, near the main terminal.  \n",
      "- **What to expect:** A fast-food joint with a variety of burgers and fries.  \n",
      "- **Why it's good:** It’s quick and efficient, perfect for a 45-minute flight.\n",
      "\n",
      "### 4. **The Bistro at Delta**  \n",
      "- **Location:** Concourse E, near the main terminal.  \n",
      "- **What to expect:** A fine dining option with a menu of gourmet dishes.  \n",
      "- **Why it's good:** It’s a good option if you’re looking for something more upscale.\n",
      "\n",
      "### 5. **Denny's**  \n",
      "- **Location:** Concourse E, near the main terminal.  \n",
      "- **What to expect:** A fast-food chain with a variety of options, including burgers and coffee.  \n",
      "- **Why it's good:** It’s fast, friendly, and has a great selection of drinks.\n",
      "\n",
      "### Tips:\n",
      "- **Check the menu** for any special offers or promotions.\n",
      "- **Avoid long lines** if possible, especially if you’re in a hurry.\n",
      "\n",
      "Let me know if you need help with anything else! 😊\n",
      "\n",
      "Finetuned Model: \n",
      "     I would suggest the Golden Wok. It's a great Chinese restaurant. Would you like me to book a table for you?\n",
      "\n",
      "Ground Truth\n",
      "     ATL is considered very efficient for its size. You can use the Plane Train, which is an automated train that runs underground, to quickly get between all concourses.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "log_dir_airport = \"/home/song0409/Desktop/CAI/data_5samples/logs_airports.json\"\n",
    "labels_dir_airport = \"/home/song0409/Desktop/CAI/data_5samples/labels_airports.json\"\n",
    "with open(log_dir_airport,'r') as f:\n",
    "    ds_airport = json.load(f)\n",
    "with open(labels_dir_airport,'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "reformatted_dataset_airport = reformat_dataset(ds_airport, labels)\n",
    "test_ds_airport = Dataset.from_dict(reformatted_dataset_airport)\n",
    "\n",
    "#test\n",
    "test_models(test_ds_airport, 5)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_cai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
